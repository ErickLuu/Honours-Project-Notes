\documentclass[]{article}
\usepackage[margin=1in]{geometry}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{url}
\usepackage{hyperref}
\usepackage[noabbrev, capitalise]{cleveref}

\usepackage[style = numeric,
isbn=false,
url=false,
eprint = false,
maxbibnames=99
]{biblatex}
\renewbibmacro{in:}{}
\DeclareSourcemap{
	\maps[datatype=bibtex]{
		\map{
			\step[fieldset=url, null]
			\step[fieldset=extra, null]
			\step[fieldset=urldate, null]
		}
	}
}
\AtEveryBibitem{%
	\clearfield{day}%
	\clearfield{month}%
	\clearfield{endday}%
	\clearfield{endmonth}%
}
\addbibresource{library.bib}
% Environments

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{conjecture}[theorem]{Conjecture}

\theoremstyle{definition}
\newtheorem{example}[theorem]{Example}

\numberwithin{theorem}{section}
\numberwithin{equation}{section}

\newcommand{\supp}{\text{Supp}}

%opening
\title{Random walks on groups Report}
\author{Eric Luu}

\begin{document}

\maketitle
\section{Introduction}

\section{Definitions}
In this section, we will outline some definitions of Markov chains and walks. This section is based on Steinberg's \cite{steinbergProbabilityRandomWalks2012}.

\subsection{Random walks on groups}
Suppose $G$ is a finite group and $X$ is a r.v. on $G$. $P$ is a probability distribution function where $P(g) = \text{Prob}[X = g]$ and $\sum_{g\in G} P(g) = 1$. We can treat this as a Markov chain but to go from $g$ to $gh$ is the same probability as going from $1$ to $h$ for all $g$ and all $h$. 

For a set $A \subseteq G$, we have that $P(A) = \sum_{g \in A} P(g)$. We define the support of $P$ as $\supp(P) = \lbrace g \in G : P(g) \neq 0 \rbrace$. 


\begin{example}
	These are examples of a random distribution on a group. 
	\begin{itemize}
		\item We define the uniform distribution $U$ on $G$ as $U(g) = 1/|G|$ for all $g \in G$. 
		\item We define the subset distribution $S \subset G$ to be $1/|S|$ for all $ g \in S$ and 0 else.
	\end{itemize}
\end{example}

Instead of treating the group as a set, we can treat the elements as functions, specifically the left regular group action. Then we can act on the group and walk around it. The convolution operator encapsulates taking steps around a group.
Let $P$, $Q$ be probabilities on $G$. Suppose we choose $X$ according to $P$ and $Y$ according to $Q$. Then the probability that $XY = g$ is the probability that $Y = h$ for some $h \in G$ and the probability that $X = gh^{-1}$. 
So $P \ast Q(g) := \text{Prob}[XY = g] = \sum_{h\in G} P(gh^{-1})Q(h) $.

\begin{proposition}
	Let $P$ and $Q$ be probability distributions on $G$. Then $P \ast Q$ is a probability distribution on $G$, with $\supp(P \ast Q) = \supp(P)\supp(Q) = \lbrace xy : x \in \supp(P), y \in \supp Q \rbrace$. 
\end{proposition}
\begin{proof}
	Trivial to show is between 0 and 1.
	Next, we have that:
	\begin{align*}
		\sum_{g \in G} P \ast Q(g) &= \sum_{g \in G} \sum_{h \in G} P(gh^{-1}) Q(h)\\
		&=\sum_{h \in G} Q(h) \sum_{g \in G} P(gh^{-1})\\
		&= \sum_{h \in G} Q(h)\\
		&= 1
	\end{align*}
\end{proof}
Thus this is a probability distribution. Obviously, the support is of the form $\left\{ xy : x \in \supp(P), y \in \supp(Q) \right\}$. 

\section{Notion of difference between probabilities}
We want to quantify how far a distribution is away from being uniform. We define the $L^1$ norm as $\|f\|_1 = \sum_{g \in G} |f(g)|$ for $f : G \rightarrow \mathbb{C}$. The $L^1$-norm is a metric and also $\|a \ast b\|_1 \leq \|a\|_1 \cdot \|b\|_1$. 

Proof in notes.

We define the total variation distance between probabilities $P$ and $Q$ on a group $G$ as $\| P - Q\|_{TV} = \sup_{A \subseteq G} |P(A) - Q(A)|$. 

\begin{lemma}
	Let $P$ and $Q$ be probabilities on $G$, and let $B = \lbrace g \in G : P(g) \geq Q(g) \rbrace$, $C = \lbrace g \in G : Q(g) \geq P(g) \rbrace$.
	Then $\|P - Q \|_{TV} = P(B) - Q(B) = Q(C) - P(C)$.
\end{lemma}
\begin{proof}[heading]
	We have that as $\sum_{g \in G} P(g) = \sum_{g \in G} Q(g) = 1$. 
	
	
	By definition, $\|P - Q\|_{TV} \geq |P(B) - Q(B)| = P(B) - Q(B)$.
	Let $A$ be a set such that $\|P - Q\|_{TV} = |P(A) - Q(A)|.$ Then $|P(A^c) - Q(A^c)| = |1 - P(A) - (1 - Q(A))| = |Q(A) - P(A)| = \|P - Q \|_{TV}$. Therefore, this is closed under complement. It is trivial to show that $P(B) - Q(B) = |Q(A) - P(A)|$. 
\end{proof}
Finally, we have that $\|P - Q \|_{TV}$ is a metric:
\begin{proposition}
	\label{prop:distribution difference equality}
	$\|P - Q\|_{TV} = \frac{1}{2} \|P - Q \|_1$. 
\end{proposition}

\begin{proof}
	\begin{align*}
		\|P - Q\|_{TV} &= \frac{1}{2}[P(B) - Q(B) + Q(C) - P(C)]\\
		&=\frac{1}{2}\left[ \sum_{g \in G : P(g) \geq Q(g)} (P(g) - Q(g)) + \sum_{g \in G : Q(g) \geq P(g)} (Q(g) - P(g)) \right]\\
		&= \frac{1}{2}\left[ \sum_{g \in G}|P(g) - Q(g)| \right]\\
		&= \frac{1}{2}\|P - Q\|_1
	\end{align*}
\end{proof}

The total variation distance is of much interest in stochastic analysis, but for ease of clarity in this report, we will use $\|P - Q\|_1$ as the canonical measure of distance between probabilities. 

We say that a sequence of probabilities $\lbrace P_n \rbrace_{n \in \mathbb{N}}$ converges to $P$ if for all $\varepsilon > 0$, there exists $N > 0$ such that $\|P_n - P \|_1 < \varepsilon$ when $n \geq N$. 
\section{Random walks}
We denote $P^{\ast k}$ as $\underbrace{P \ast P \ast ... \ast P}_{k \text{times}}$. 
The way to think about random walks is as follows. We start at the identity and move to element $x_1$ according to $P$. Then we choose another element $x_2$ and move to $x_1 x_2$. So this is a sequence of i.i.d. variables with distribution $P$.

This forms a Markov chain. 

\begin{example}
	Examples of random walks on graphs
	\begin{itemize}
		\item Let $G$ be a group and $R$ be the generators and their inverses. Let $\Gamma$ be the Cayley graph of $G$. The simple random walk is to choose a random element $s \in R$ with uniform distribution and move to the vertex $gs$. By construction, the vertices adjacent to $\Gamma$ on $g$ are $gs$ with $s \in R$.
		\item Let $G = \mathbb{Z}_n$. Let us walk  $p, q \geq 0$, $p + q = 1$. Suppose we have that $P[X = 1] = p$, $P[x = -1] = q$ Then we go one step clockwise with probability $p$ and one step back with probability $q$. Then the probability distribution is $p \delta_1 + q \delta_{-1}$. 
	\end{itemize}
\end{example}

\subsection{Ergodicity}
A random walk on a group $G$ driven by a probability $P$ is said to be ergodic if there exists an integer $N > 0$ such that $P^{\ast N}(g) > 0$ for all $g \in G$, so every event can be reached from $P$. 

\subsection{Random walks}
We define a Markov chain on a set $\mathcal{X}$ in the following manner. We have a fixed probability distribution at each fixed point in $x$, so $P_x : \mathcal{X} \rightarrow [0, 1]$ is a probability distribution. 

We say a sequence of random variables $(X_0, X_1, ...)$ is a Markov chain with state space $\mathcal{X}$ and distribution $P$ if 
\begin{equation}
	P_x(y) = P(X_{t + 1} = y | X_t = x)
\end{equation}

We will denote the Markov probability as $P$ where $P(x, y) = P_x(y)$. Typically $P$ is represented as a matrix where $[P]_{i,j} = P(i, j)$. 

\subsubsection{Stationary distributions}
We wish to see what the stationary distribution converges to in the long-run. This means that after a set time $N$, we wish to see the proportion of time the variable stays in state $x \in \mathcal{X}$. This is given by the state distribution $\pi$. 

We have that $\pi$ satisfies $\pi = \pi P$. 

\subsubsection{Markov chains, in general}
In standard Markov chain language, we say that a Markov chain is \textit{irreducible} if we can reach one state from another. A state $i$ has \textit{period} $k$ if $k$ is the gcd of all times that you can take to go back to $i$ if you were at $i$ at time $t = 0$. If $k = 1$, then we say $i$ is \textit{aperiodic}. 

A state $i$ is \textit{transient} if there is a nonzero probability that we will never reach $i$, and \textit{recurrent} otherwise. A state $i$ is \textit{positive recurrent} if the expected time to reach $i$ is finite.

We say a Markov chain is \textit{ergodic} if it is both \textit{aperiodic} and \textit{positive recurrent}. 


\begin{proposition}
	Let $P$ be a probability on a finite group $G$ and suppose $P(1) > 0$ and $Supp(P)$ generates the group $G$. Then the random walk driven by $P$ is ergodic.
\end{proposition}

\begin{proof}
	Let $S = Supp(P)$. As $S$ is a generating set, then there exists an $N$ such that $S^N = G$. So we have that $P^{\ast N} g > 0$ by the definition of the support. Finally, $S^{k} \subseteq S^{k + 1}$ as $1 \in S$. Therefore, $P$ is ergodic. 
\end{proof}
\section{Fourier analysis on finite groups}
\subsection{Dual groups}
Let $G$ be a finite abelian group and let $\widehat{G}$ be the set of irreducible characters $\chi: G \rightarrow \mathbb{C}$. Note all characters are of degree 1, so they are representations of degree 1.We will use the dual character/representation multiple times. $\widehat{G}$ is the dual group of $G$, where multiplication is defined as $(\chi_{ab})(g) = \chi_a(g)\chi_b(g)$, inverses are defined as $\chi_a^{-1} = \overline{\chi_a}$, and $\chi_1$, the trivial group, is the identity. (Prove later!).

\begin{theorem}
	Let $G$ be a finite abelian group. Then $G \cong \widehat{G}$.
\end{theorem}
This is easy to see for cyclic groups $\mathbb{Z}_n$, as for any generator $\langle 1 \rangle$, we send $\rho_a(1) = e^{2 i \pi a / n}$. Now for any finite cyclic group $\mathbb{Z}_{\alpha} \times \mathbb{Z}_{\beta}... $ (we can do this using the characterisation of abelian groups) with generators $\langle 1_{\alpha}, 1_\beta... \rangle$, we look at the irreducible representations on just $\mathbb{Z}_{i}$ and take the tensor product with all the other representations on $\mathbb{Z}_j$ for any different $j$ in the group product to form the character group. 

We have the general definition of Fourier transform. Let $f \in ZL(G)$. Then $\widehat{f}: \widehat{G} \rightarrow \mathbb{C}$ is defined as:
\begin{equation}
	\widehat{f}(\chi) = |G| \langle f, \chi \rangle = \sum_{g \in G} f(g) \overline{\chi(g)}
\end{equation}

The Fourier inverse is defined as:

\begin{equation}
	f(x) = \frac{1}{|G|} \sum_{\chi \in \widehat{G}} \widehat{f}(\chi) \chi(x)
\end{equation}

Note that $ZL(G) = L(G)$, but we will use the former more often than not to generalise to non-abelian groups.

Furthermore, we have that if $\chi, \psi \in \widehat{G}$, then $\widehat{\chi}(\psi) = \langle \chi, \psi \rangle = |G|\delta_{\chi, \psi}$. So $\widehat{\chi} = |G| \delta_{\chi}$, where $\delta_{\chi}(\psi) = 1$ if $\psi = \chi$ and 0 else. 

\begin{theorem}
	\label{thm:abelian dual commutation}
	Let $G$ be an abelian group, and $a, b \in ZL(G)$. Then $ \widehat{a \ast b} = \widehat{a} \widehat{b}$. 
\end{theorem}

\begin{proof}
	We have that:
\begin{align*}
		\widehat{a \ast b}(\chi) &= |G| \langle a \ast b, \chi \rangle\\
		&= \sum_{g \in G} (a \ast b) (g) \overline{\chi(g)} \\
		&= \sum_{g \in G} \overline{\chi(g)} 
		\sum_{h \in G} a(g h^{-1}) b(h) \\
		&= \sum_{h \in G} b(h)  
		\sum_{g \in G} a(g h^{-1})\overline{\chi(g)}  
\end{align*} 
And setting $k = gh^{-1}$, so $g =kh$, we get that:
\begin{align*}
		&= \sum_{h \in G} b(h)  
		\sum_{k \in G} a(k)\overline{\chi(kh)} \\
		&= \sum_{h \in G} b(h)  
		\sum_{k \in G} a(k)\overline{\chi(k)} \overline{\chi(h)} \\
		&=  \sum_{h \in G} b(h)  \overline{\chi(h)} 
		\sum_{k \in G} a(k)\overline{\chi(k)}\\
		&= \widehat{a}(\chi) \widehat{b}(\chi)
\end{align*}
\end{proof}

\begin{theorem}
	\label{thm:Eigenvector operators}
	Let $G$ be a finite abelian group and $a \in \mathbb{C}(G)$. Define the operator $A: \mathbb{C}(G) \rightarrow \mathbb{C}(G)$ by $A(b) = a \ast b$. Then $A$ is linear and furthermore $\chi$ is an eigenvector with eigenvalue $\widehat{a}(\chi)$ for all $\chi \in \widehat{G}$. Therefore, $A$ is diagonalisable. 
\end{theorem}

\begin{proof}
	First, we have that $\widehat{a\ast \chi} = \widehat{a} \widehat{\chi} = \widehat{a} |G| \delta_\chi$, and $\widehat{a} |G| \delta_\chi(\psi) = \widehat{a}(\psi) |G|$ if $\psi = \chi$ and 0 else, so $\widehat{a} |G| \delta_\chi = \widehat{a}(\chi) |G| \delta_\chi$. When we apply the inverse of the Fourier transform, we have that $a \ast \chi = \widehat{a}(\chi) \chi$ by the 1st orthogonality, so $A \chi = \widehat{a}(\chi) \chi$. Thus shown. 
\end{proof}

\section{Spectrums of random walks}
We define a convolution operator on a probability distribution $P$ as $M : \mathbb{C}G \rightarrow \mathbb{C}G$ where $M(a) = P \ast a$, where $(P \ast a) (g) =\sum_{h\in G} P(gh^{-1})a(h)$.
\begin{definition}
	The spectrum of a random walk driven by probability distribution $P$ is the set of eigenvalues of $M$. 
\end{definition} 
It is easy to show that $P \ast U = U$ and that $U$ is an eigenvector of $M$ with eigenvalue $1$. 


\begin{theorem}[Diaconis]
	\label{thm:Diaconis}
	Let $G$ be a finite abelian group and let $P$ be a probability on $G$. Then $spec(P) := \lbrace \widehat{P}(\chi) : \chi \in \widehat{G} \rbrace$ where the multiplicity of $\lambda \in spec(P)$ are the number of characters where $\widehat{P}(\chi) = \lambda$. The orthonormal basis for the eigenspace of $\lambda$ is the set of all characters $\chi$ where $\widehat{P}(\chi) = \lambda$. 
\end{theorem}
This is a special case of \cref{thm:Eigenvector operators}.

\begin{lemma}
	\label{lem:Absolute Value Inequality}
	Define $\|f\| = \sqrt{\langle f, f \rangle}$, where $f \in \mathbb{C}(G)$. Then $\|f \|_1 \leq |G| \|f \|$.
\end{lemma}
\begin{proof}
	We have that $\|f \|_1 = |G| \langle f, \chi_1 \rangle \leq |G| \|f \| \|\chi_1 \| = |G| \|f \|$, where $\|\chi_1 \| = 1$ and we use the Cauchy-Schwartz inequality. 
\end{proof}

\begin{theorem}[Plancherel]
	\label{thm:Plancherel}
	Let $G$ be a finite group and let $a, b \in \mathbb{C}(G)$. Then $\langle a, b \rangle = \frac{1}{|G|} \langle \widehat{a}, \widehat{b} \rangle$. 
\end{theorem}
\begin{proof}
	We have that $a =\frac{1}{|G|} \sum_{\chi \in \widehat{G}} \widehat{a}(\chi) \chi$, and $b = \frac{1}{|G|} \sum_{\chi \in \widehat{G}} \widehat{b}(\chi) \chi$. Thus \begin{equation}
		\langle a, b \rangle = \left(\frac{1}{|G|}\right)^2 \langle \widehat{a}(\chi), \widehat{b}(\chi) \rangle \delta_{\chi, \chi} = \frac{1}{|G|} \langle \widehat{a}, \widehat{b} \rangle.
	\end{equation}

\end{proof}

\begin{lemma}[Diaconis–Shahshahan Upper bound lemma]
	\label{lem:Upper bound lemma}
	Let $G$ be a finite abelian group, and let $F$ be the set of non-trivial irreducible characters of $G$. Let $Q$ be a probability on $G$.  Then:
	\begin{equation}
		\|Q - U\|^2_{1} \leq \sum_{\chi \in F} |\widehat{Q}(\chi)|^2.
	\end{equation}
\end{lemma}
\begin{proof}[Proof]

	and from \cref{lem:Absolute Value Inequality}, we have that:
	\begin{equation}
		 \| P - Q\|_1^2 \leq |G|^2 \| Q - U \|^2
	\end{equation}
	Then by \cref{thm:Plancherel},
	\begin{equation}
		|G|^2 \|Q - U \|^2 = |G| \left[ \langle \widehat{Q}, \widehat{Q} \rangle - 2 \langle \widehat{Q}, \widehat{U} \rangle + \langle \widehat{U}, \widehat{U} \rangle \right]
	\end{equation}
	As $\widehat{U} = \delta_{\chi_1}$, then $\widehat{U}(\chi) = \langle \chi_1, \chi \rangle$. 
	Therefore, $\langle \chi_1, \chi_1 \rangle = \frac{1}{|G|}$, and $\langle \widehat{Q}, \widehat{U} = \widehat{Q}(\chi_1) = 1/|G|$, and
	\begin{equation}
		\langle \widehat{Q}, \widehat{Q} \rangle =\frac{1}{|G|} + \frac{1}{|G|} \sum_{\chi \in F} \widehat{Q}(\chi) \overline{\widehat{Q}(\chi)}.
	\end{equation}
	Therefore, $|G|^2 \|Q - U \|^2 =  \sum_{\chi \in F}| \widehat{Q}(\chi)|^2$ and thus 
	\begin{equation}
		\|Q - U\|^2_{1} \leq \sum_{\chi \in F} |\widehat{Q}(\chi)|^2.
	\end{equation}
\end{proof}

\begin{corollary}
	\label{cor:Convolution Inequality}
	Let $G$ be a finite abelian group and let $P$ be a probability on $G$. Then 
	\begin{equation}
	\|P^{\ast k} - U\|^2_{1} \leq \sum_{\chi \in F} |\widehat{P}(\chi)|^{2k}.
\end{equation}
\end{corollary}

\begin{theorem}
	If $P$ is a probability on a finite abelian group $G$ and $P$ is ergodic, then $P^{\ast n}$ converges to $U$.
\end{theorem}
It suffices to show that $|\widehat{P}(\chi)| < 1$ for all nontrivial characters $\chi$ of $G$. Assume that $P(g) > 0$ with ergodicity. Recall that $|\widehat{P}(\chi)| = |\sum_{g \in G} P(G) \overline{\chi(g)}| \leq \sum_{g \in G} P(G) |\overline{\chi(g)}| = 1$ But we have that if $|\widehat{P}(\chi)| = 1$, then $P(g) \overline{\chi(g)}$ are non-negative multiples of a common complex number. But $P(1) \geq 0$ and $\overline{\chi(1)} = 1$. Therefore, $P(1) \overline{\chi(1)} = P(1)$, but we have that there is a $\overline{\chi(g)} \neq 1$ for some $g \in G$. Thus we have that equality does not hold, so $|\widehat{P}(\chi)| < 1$. 

\subsection{Discussion}
This result means that all random ergodic walks on groups that respect the group operation converge to a uniform distribution. Note that we have for any general Markov chain, then aperiodic and irreducible walks converge to some distribution, rather than a uniform distribution. This assumption is much weaker than on any set as we do not need aperiodicity to show convergence, but we do need the walk to respect the fact that groups are transitive. We use the fact that the group is finite abelian because the characters are very well defined: they are degree 1 and we can take tensor products to make characters for every element of the group in the natural way. 

In fact, there are similar results for groups that are nonabelian. We can drop some of the assumptions and have similar results for non-abelian groups.

\section{Examples}
\subsection{Ehrenfest's urn}
Suppose we have two urns, labelled $A$ and $B$, and urn $A$ contains $n$ balls and $B$ contains no balls. Then at each step in time, we choose a random ball and move it to the other urn. This is the same as acting on $(\mathbb{Z}_2)^n$ such that the states that you can reach from $g \in (\mathbb{Z}_2)^n$ are to flip one of the bits with uniform probability. 

\begin{theorem}
	Let $c > 0$ be a positive constant. Then there exists a sufficiently large $k$ such that:
	\begin{equation}
		\|P^{\ast k} - U\|^2_{TV} \leq \frac{1}{2} (e^{e^{-c}} - 1)
	\end{equation}
\end{theorem}

\subsection{Card shuffling}
Shuffling $n$ cards with a riffle-shuffle has a sharp cutoff at $\frac{3}{2} n \log n$ shuffles. 
\section{Conclusion}
We first discussed what a Markov chain is and what a stationary distribution looks like on a Markov chain. We then looked at what a random walk on a group is and how it connected to Markov chains. We also looked at how we could test if two Markov chains are close to each other. We then looked specifically at the case where the group is a finite abelian group. Abelian groups of characters are very easy to compute if you know the basis elements. We used Fourier analysis to prove a series of results on abelian groups of characters, and eventually showed an upper bound of the of the distance between a limiting ergodic distribution and the uniform distribution can get arbitrarily small, thus showing that any ergodic distribution converges to the uniform distribution. This result is much more helpful than the stationary distribution because it required less axioms (distribution does not have to be aperiodic) yet converged to a particular stationary distribution, the uniform distribution. We finally finished by looking at some examples. 
\end{document}
