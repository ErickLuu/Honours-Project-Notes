\documentclass[]{article}
\usepackage[margin=1in]{geometry}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{url}
\usepackage{todonotes}
\usepackage{hyperref}
\usepackage[noabbrev, capitalise]{cleveref}

\usepackage[style = numeric,
isbn=false,
url=false,
eprint = false,
maxbibnames=99
]{biblatex}
\renewbibmacro{in:}{}
\DeclareSourcemap{
	\maps[datatype=bibtex]{
		\map{
			\step[fieldset=url, null]
			\step[fieldset=extra, null]
			\step[fieldset=urldate, null]
		}
	}
}
\AtEveryBibitem{%
	\clearfield{day}%
	\clearfield{month}%
	\clearfield{endday}%
	\clearfield{endmonth}%
}
\addbibresource{library.bib}
% Environments

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{conjecture}[theorem]{Conjecture}

\theoremstyle{definition}
\newtheorem{example}[theorem]{Example}

\numberwithin{theorem}{section}
\numberwithin{equation}{section}

\newcommand{\supp}{\text{Supp}}

%opening
\title{Random walks on groups Report}
\author{Eric Luu}

\begin{document}

\maketitle
\section{Introduction}


One of the most famous works on random walks on groups is \cite{bayerTrailingDovetailShuffle1992} by Bayer and Diaconis. 
\section{Definitions}
In this section, we will outline some definitions of Markov chains and walks. This section is based on Steinberg's chapter on random walks\cite{steinbergProbabilityRandomWalks2012}.

\subsection{Random walks on groups}
Suppose $G$ is a finite group and $X$ is a r.v. on $G$. $P$ is a probability distribution function where $P(g) = \text{Prob}[X = g]$ and $\sum_{g\in G} P(g) = 1$. We can treat this as a Markov chain but to go from $g$ to $gh$ is the same probability as going from $1$ to $h$ for all $g$ and all $h$. 

For a set $A \subseteq G$, we have that $P(A) = \sum_{g \in A} P(g)$. We define the support of $P$ as $\supp(P) = \lbrace g \in G : P(g) \neq 0 \rbrace$. 


\begin{example}
	These are examples of a random distribution on a group. 
	\begin{itemize}
		\item We define the uniform distribution $U$ on $G$ as $U(g) = 1/|G|$ for all $g \in G$. 
		\item We define the subset distribution $S \subset G$ to be $1/|S|$ for all $ g \in S$ and 0 else.
	\end{itemize}
\end{example}

Instead of treating the group as a set, we can treat the elements as functions, specifically the left regular group action. Then we can act on the group and walk around it. The convolution operator encapsulates taking steps around a group.
Let $P$, $Q$ be probabilities on $G$. Suppose we choose $X$ according to $P$ and $Y$ according to $Q$. Then the probability that $XY = g$ is the probability that $Y = h$ for some $h \in G$ and the probability that $X = gh^{-1}$. 
So $P \ast Q(g) := \text{Prob}[XY = g] = \sum_{h\in G} P(gh^{-1})Q(h) $.

\begin{proposition}
	Let $P$ and $Q$ be probability distributions on $G$. Then $P \ast Q$ is a probability distribution on $G$, with $\supp(P \ast Q) = \supp(P)\supp(Q) = \lbrace xy : x \in \supp(P), y \in \supp Q \rbrace$. 
\end{proposition}
\begin{proof}
	Trivial to show is between 0 and 1.
	Next, we have that:
	\begin{align*}
		\sum_{g \in G} P \ast Q(g) &= \sum_{g \in G} \sum_{h \in G} P(gh^{-1}) Q(h)\\
		&=\sum_{h \in G} Q(h) \sum_{g \in G} P(gh^{-1})\\
		&= \sum_{h \in G} Q(h)\\
		&= 1
	\end{align*}
\end{proof}
Thus this is a probability distribution. Obviously, the support is of the form $\left\{ xy : x \in \supp(P), y \in \supp(Q) \right\}$. 

\section{Random walks}
We denote $P^{\ast k}$ as $\underbrace{P \ast P \ast ... \ast P}_{k \text{times}}$. 
The way to think about random walks is as follows. We start at the identity and move to element $x_1$ according to $P$. Then we choose another element $x_2$ and move to $x_1 x_2$. So this is a sequence of i.i.d. variables with distribution $P$.

This forms a Markov chain. 

\begin{example}
	Examples of random walks on graphs
	\begin{itemize}
		\item Let $G$ be a group and $R$ be the generators and their inverses. Let $\Gamma$ be the Cayley graph of $G$. The simple random walk is to choose a random element $s \in R$ with uniform distribution and move to the vertex $gs$. By construction, the vertices adjacent to $\Gamma$ on $g$ are $gs$ with $s \in R$.
		\item Let $G = \mathbb{Z}_n$. Let us walk  $p, q \geq 0$, $p + q = 1$. Suppose we have that $P[X = 1] = p$, $P[x = -1] = q$ Then we go one step clockwise with probability $p$ and one step back with probability $q$. Then the probability distribution is $p \delta_1 + q \delta_{-1}$. 
	\end{itemize}
\end{example}

\subsection{Ergodicity}
A random walk on a group $G$ driven by a probability $P$ is said to be ergodic if there exists an integer $N > 0$ such that $P^{\ast N}(g) > 0$ for all $g \in G$, so every event can be reached from $P$. 

\subsection{Random walks}
We define a Markov chain on a set $\mathcal{X}$ in the following manner. We have a fixed probability distribution at each fixed point in $x$, so $P_x : \mathcal{X} \rightarrow [0, 1]$ is a probability distribution. 

We say a sequence of random variables $(X_0, X_1, ...)$ is a Markov chain with state space $\mathcal{X}$ and distribution $P$ if 
\begin{equation}
	P_x(y) = P(X_{t + 1} = y | X_t = x)
\end{equation}

We will denote the Markov probability as $P$ where $P(x, y) = P_x(y)$. Typically $P$ is represented as a matrix where $[P]_{i,j} = P(i, j)$. 

\subsubsection{Definitions}
In standard Markov chain language, we say that a Markov chain is \textit{irreducible} if we can reach one state from another. A state $i$ has \textit{period} $k$ if $k$ is the gcd of all times that you can take to go back to $i$ if you were at $i$ at time $t = 0$. If $k = 1$, then we say $i$ is \textit{aperiodic}. 

A state $i$ is \textit{transient} if there is a nonzero probability that we will never reach $i$, and \textit{recurrent} otherwise. A state $i$ is \textit{positive recurrent} if the expected time to reach $i$ is finite. On a finite set $X$, recurrent implies positive recurrent. 

We say a Markov chain is \textit{ergodic} if every state is \textit{positive recurrent}. 
\subsubsection{Stationary distributions}
We wish to see what the distribution converges to in the long-run. We define a stationary distribution $\pi$ to be a vector on a transition matrix $P$ such that $\pi P = \pi$. 

\begin{theorem}
	If a Markov chain is irreducible and aperiodic, then there exists a unique stationary distribution $\pi$, and $P^{*k}$ converges to a rank one matrix where every row is $\pi$. 
\end{theorem}


\subsection{Results for finite groups}
For probability distributions on groups, we have some stronger statements. In fact, we will show that $\pi$ is uniform, and in fact the ergodicity of the walk is easier to show with a group. 
\begin{proposition}
	Let $P$ be a probability on a finite group $G$ and suppose $P(1) > 0$ and $Supp(P)$ generates the group $G$. Then the random walk driven by $P$ is ergodic.
\end{proposition}

\begin{proof}
	Let $S = Supp(P)$. As $S$ is a generating set, then there exists an $N$ such that $S^N = G$. So we have that $P^{\ast N} g > 0$ by the definition of the support. Finally, $S^{k} \subseteq S^{k + 1}$ as $1 \in S$. Therefore, $P$ is ergodic. 
\end{proof}

\section{Notion of difference between probabilities}
We want to quantify how far a distribution is away from being uniform. We define the $L^1$ norm as $\|f\|_1 = \sum_{g \in G} |f(g)|$ for $f : G \rightarrow \mathbb{C}$. The $L^1$-norm is a metric and also $\|a \ast b\|_1 \leq \|a\|_1 \cdot \|b\|_1$. 

Proof in notes.

We define the total variation distance between probabilities $P$ and $Q$ on a group $G$ as $\| P - Q\|_{TV} = \sup_{A \subseteq G} |P(A) - Q(A)|$. 

\begin{lemma}
	Let $P$ and $Q$ be probabilities on $G$, and let $B = \lbrace g \in G : P(g) \geq Q(g) \rbrace$, $C = \lbrace g \in G : Q(g) \geq P(g) \rbrace$.
	Then $\|P - Q \|_{TV} = P(B) - Q(B) = Q(C) - P(C)$.
\end{lemma}
\begin{proof}[heading]
	We have that as $\sum_{g \in G} P(g) = \sum_{g \in G} Q(g) = 1$. 
	
	
	By definition, $\|P - Q\|_{TV} \geq |P(B) - Q(B)| = P(B) - Q(B)$.
	Let $A$ be a set such that $\|P - Q\|_{TV} = |P(A) - Q(A)|.$ Then $|P(A^c) - Q(A^c)| = |1 - P(A) - (1 - Q(A))| = |Q(A) - P(A)| = \|P - Q \|_{TV}$. Therefore, this is closed under complement. It is trivial to show that $P(B) - Q(B) = |Q(A) - P(A)|$. 
\end{proof}
Finally, we have that $\|P - Q \|_{TV}$ is a metric:
\begin{proposition}
	\label{prop:distribution difference equality}
	$\|P - Q\|_{TV} = \frac{1}{2} \|P - Q \|_1$. 
\end{proposition}

\begin{proof}
	\begin{align*}
		\|P - Q\|_{TV} &= \frac{1}{2}[P(B) - Q(B) + Q(C) - P(C)]\\
		&=\frac{1}{2}\left[ \sum_{g \in G : P(g) \geq Q(g)} (P(g) - Q(g)) + \sum_{g \in G : Q(g) \geq P(g)} (Q(g) - P(g)) \right]\\
		&= \frac{1}{2}\left[ \sum_{g \in G}|P(g) - Q(g)| \right]\\
		&= \frac{1}{2}\|P - Q\|_1
	\end{align*}
\end{proof}

The total variation distance is of much interest in stochastic analysis, but for ease of clarity in this report, we will use $\|P - Q\|_1$ as the canonical measure of distance between probabilities. 

We say that a sequence of probabilities $\lbrace P_n \rbrace_{n \in \mathbb{N}}$ converges to $P$ if for all $\varepsilon > 0$, there exists $N > 0$ such that $\|P_n - P \|_1 < \varepsilon$ when $n \geq N$. 

\section{Fourier analysis on finite groups}
In this section, we will prove some important ideas about Fourier analysis and finite groups. We will mainly be using Steinberg \cite{steinbergProbabilityRandomWalks2012} for finite abelian groups, but we will be using a paper from Diaconis and Shahshahani's paper on random groups \cite{diaconisGeneratingRandomPermutation1981} for some results on all finite groups. 

\begin{definition}
	If $\rho$ is a representation and $P$ is a function on $G$, then
	\begin{equation}
		\rho(P) = \sum_{g \in G} P(g) \rho(g)
	\end{equation}
\end{definition}

This is like a Fourier transform. It converts the convolution operator into multiplication.

\begin{lemma}\label{lem:rep_ast}
	If $P, Q$ are functions on $G$ and $\rho$ is a representation, then $\rho(P \ast Q) = \rho(P) \rho(Q)$.
\end{lemma}

\begin{proof}
	We have that:
	\begin{align*}
		\rho(P \ast Q) &= \sum_{g \in G} \rho(g) (P \ast Q)(g)\\
		&= \sum_{g \in G} \rho(g) \sum_{h \in G} P(g h^{-1}) Q(h)\\
		&= \sum_{g \in G}\sum_{h \in G} \rho(gh^{1}) \rho(h)  P(g h^{-1}) Q(h)\\
		&= \left(\sum_{h \in G}  \rho(h) Q(h)\right) \sum_{g \in G} \rho(gh^{1})P(g h^{-1}) \quad \text{ Let } x = gh^{-1} \\
		&= \rho(Q) \sum_{x \in G} \rho(x) P(x)\\
		&= \rho(Q) \rho(P)
	\end{align*}
	As shown. 
\end{proof}

For class functions, we have a stronger result.

\begin{theorem}\label{thm:class_fns}
	Let $G$ be a finite group and $\rho$ an irrep of $G$. let $P$ be a class function and let $C = \lbrace C_1... C_n \rbrace$ be the conjugacy classes with $\lbrace c_1, ..., c_n \rbrace$ the associated representatives in $G$. Then:
	\begin{equation}
		\rho(P) = C \times Id \qquad \text{where} \qquad C = \frac{1}{\deg(\rho)}  \sum_{i = 1}^n\left(|C_i|\right) P(c_i) \chi_\rho(c_i).
	\end{equation}
\end{theorem}
\begin{proof}
	We will use Schur's lemma to prove this result. 
	Let $M_i = \sum_{c \in C_i} \rho(c)$. Then we have that:
	\begin{equation}
		\rho(P) = \sum_{g \in G} P(g) \rho(g) = \sum_{i = 1}^n P(c_i) M_i
	\end{equation}
	We have that $\rho(g) \left(\sum_{c \in C_i} \rho(c)\right) \rho(g^{-1}) = M_i$ for all $g \in G$. Therefore, we use Schur's lemma to say that $M_i = K_i I$ for some real number $K_i$. Then by taking the trace, we have that:
	\begin{equation}
		Tr(M_i) = |C_i| \chi_\rho(c_i) = K_i \deg(\rho).
	\end{equation}
	Thus shown. 
\end{proof}

We will finish with an example by Diaconis and Shahshahani. This example involves swapping two cards in the deck with uniform probability, and is possibly the simplest shuffle on any deck. \cite{diaconisGeneratingRandomPermutation1981}
\begin{example}
	Let $T$ be a probability measure on $S_n$ defined as such:
	\begin{equation}
		T(\sigma) = \begin{cases}
			\frac{1}{n} &, \sigma = 1\\
			\frac{2}{n^2} &, \sigma = \text{ transposition}\\
			0 &, \text{else}
		\end{cases}
	\end{equation}
	Then $T$ is a class function, and from \cref{thm:class_fns}, we have that for $\rho \in S_n$:
	\begin{equation}
		\rho(T) = \left(\frac{1}{n} + \frac{n-1}{n} \frac{\chi_\rho(\tau)}{\deg(\rho)}\right) Id.
	\end{equation}
\end{example}
We have that $\chi(1) = \deg(\rho)$, so this pops the first one out. Next, we have $|C_{\text{transpositions}}| = \binom{n}{2} = \frac{n(n-1)}{2}$, so we have the result above.
\subsection{Dual groups}
Let $G$ be a finite group and let $\widehat{G}$ be the set of irreducible characters $\chi: G \rightarrow \mathbb{C}$. If $G$ is also abelian, then we have a stronger result. 
\begin{theorem}
	If $G$ is abelian, then $\widehat{G}$ is a group where multiplication is defined as $(\chi_{ab})(g) = \chi_a(g)\chi_b(g)$, inverses are defined as $\chi_a^{-1} = \overline{\chi_a}$, and $\chi_1$, the trivial group, is the identity. 
\end{theorem}
\begin{proof}
	We have that $\widehat{G}$ are characters of degree 1.
	 We have that for $\chi_a \in \widehat{G}$, $\chi_a(g)$ is a root of unity of $|g|$. Then multiplying two characters also yields a root of unity. To show that the multiplication of two elements in $\widehat{G}$ yields another character, recall that $\rho_{ab} = \rho_a \otimes \rho_b$ implies that $\chi_{ab} = \chi_a \times \chi_b$. But as these characters are representations of degree 1, then multiplying elements is the same as taking the tensor products of their respective representations. But since the tensor products are also of degree 1, then they must also be characters in $\widehat{G}$. 
\end{proof}
What is the most important part about $G$ being abelian is that the representations of $G$ are all of degree $1$, so characters are representations. 
\begin{theorem}
	Let $G$ be a finite abelian group. Then $G \cong \widehat{G}$.
\end{theorem}
\begin{proof}
	This is easy to see for cyclic groups $\mathbb{Z}_n$, as for any generator $\langle k \rangle$, we send $\rho_a(k) = e^{2 i \pi a / n}$. Now for any finite cyclic group $\mathbb{Z}_{\alpha} \times \mathbb{Z}_{\beta}... $ (we can do this using the characterisation of abelian groups) with generators $\langle 1_{\alpha}, 1_\beta... \rangle$, we look at the irreducible representations on just $\mathbb{Z}_{i}$ and take the tensor product with all the other representations on $\mathbb{Z}_j$ for any different $j$ in the group product to form the character group. 
\end{proof}

We have the general definition of Fourier transform. Let $f \in ZL(G)$. Then $\widehat{f}: \widehat{G} \rightarrow \mathbb{C}$ is defined as:
\begin{equation}
	\widehat{f}(\chi) = |G| \langle f, \chi \rangle = \sum_{g \in G} f(g) \overline{\chi(g)}
\end{equation}

The Fourier inverse is defined as:

\begin{equation}
	f(x) = \frac{1}{|G|} \sum_{\chi \in \widehat{G}} \widehat{f}(\chi) \chi(x)
\end{equation}

Note that $ZL(G) = L(G)$, but we will use the former more often than not to generalise to non-abelian groups.

Furthermore, we have that if $\chi, \psi \in \widehat{G}$, then $\widehat{\chi}(\psi) = \langle \chi, \psi \rangle = |G|\delta_{\chi, \psi}$. So $\widehat{\chi} = |G| \delta_{\chi}$, where $\delta_{\chi}(\psi) = 1$ if $\psi = \chi$ and 0 else. 

\begin{theorem}
	\label{thm:abelian dual commutation}
	Let $G$ be an abelian group, and $a, b \in ZL(G)$. Then $ \widehat{a \ast b} = \widehat{a} \widehat{b}$. 
\end{theorem}

\begin{proof}
	We have that:
\begin{align*}
		\widehat{a \ast b}(\chi) &= |G| \langle a \ast b, \chi \rangle\\
		&= \sum_{g \in G} (a \ast b) (g) \overline{\chi(g)} \\
		&= \sum_{g \in G} \overline{\chi(g)} 
		\sum_{h \in G} a(g h^{-1}) b(h) \\
		&= \sum_{h \in G} b(h)  
		\sum_{g \in G} a(g h^{-1})\overline{\chi(g)}  
\end{align*} 
And setting $k = gh^{-1}$, so $g =kh$, we get that:
\begin{align*}
		&= \sum_{h \in G} b(h)  
		\sum_{k \in G} a(k)\overline{\chi(kh)} \\
		&= \sum_{h \in G} b(h)  
		\sum_{k \in G} a(k)\overline{\chi(k)} \overline{\chi(h)} \\
		&=  \sum_{h \in G} b(h)  \overline{\chi(h)} 
		\sum_{k \in G} a(k)\overline{\chi(k)}\\
		&= \widehat{a}(\chi) \widehat{b}(\chi)
\end{align*}
\end{proof}

\begin{theorem}
	\label{thm:Eigenvector operators}
	Let $G$ be a finite abelian group and $a \in \mathbb{C}(G)$. Define the operator $A: \mathbb{C}(G) \rightarrow \mathbb{C}(G)$ by $A(b) = a \ast b$. Then $A$ is linear and furthermore $\chi$ is an eigenvector with eigenvalue $\widehat{a}(\chi)$ for all $\chi \in \widehat{G}$. Therefore, $A$ is diagonalisable. 
\end{theorem}

\begin{proof}
	First, we have that $\widehat{a\ast \chi} = \widehat{a} \widehat{\chi} = \widehat{a} |G| \delta_\chi$, and $\widehat{a} |G| \delta_\chi(\psi) = \widehat{a}(\psi) |G|$ if $\psi = \chi$ and 0 else, so $\widehat{a} |G| \delta_\chi = \widehat{a}(\chi) |G| \delta_\chi$. When we apply the inverse of the Fourier transform, we have that $a \ast \chi = \widehat{a}(\chi) \chi$ by the 1st orthogonality, so $A \chi = \widehat{a}(\chi) \chi$. Thus shown. 
\end{proof}

\section{Spectrums of random walks}
We define a convolution operator on a probability distribution $P$ as $M : \mathbb{C}G \rightarrow \mathbb{C}G$ where $M(a) = P \ast a$, where $(P \ast a) (g) =\sum_{h\in G} P(gh^{-1})a(h)$.
\begin{definition}
	The spectrum of a random walk driven by probability distribution $P$ is the set of eigenvalues of $M$. 
\end{definition} 
It is easy to show that $P \ast U = U$ and that $U$ is an eigenvector of $M$ with eigenvalue $1$. 


\begin{theorem}[Diaconis]
	\label{thm:Diaconis}
	Let $G$ be a finite abelian group and let $P$ be a probability on $G$. Then $spec(P) := \lbrace \widehat{P}(\chi) : \chi \in \widehat{G} \rbrace$ where the multiplicity of $\lambda \in spec(P)$ are the number of characters where $\widehat{P}(\chi) = \lambda$. The orthonormal basis for the eigenspace of $\lambda$ is the set of all characters $\chi$ where $\widehat{P}(\chi) = \lambda$. 
\end{theorem}
This is a special case of \cref{thm:Eigenvector operators}.

\begin{lemma}
	\label{lem:Absolute Value Inequality}
	Define $\|f\| = \sqrt{\langle f, f \rangle}$, where $f \in \mathbb{C}(G)$. Then $\|f \|_1 \leq |G| \|f \|$.
\end{lemma}
\begin{proof}
	We have that $\|f \|_1 = |G| \langle f, \chi_1 \rangle \leq |G| \|f \| \|\chi_1 \| = |G| \|f \|$, where $\|\chi_1 \| = 1$ and we use the Cauchy-Schwartz inequality. 
\end{proof}

\begin{theorem}[Plancherel]
	\label{thm:Plancherel}
	Let $G$ be a finite group and let $a, b \in \mathbb{C}(G)$. Then $\langle a, b \rangle = \frac{1}{|G|} \langle \widehat{a}, \widehat{b} \rangle$. 
\end{theorem}
\begin{proof}
	We have that $a =\frac{1}{|G|} \sum_{\chi \in \widehat{G}} \widehat{a}(\chi) \chi$, and $b = \frac{1}{|G|} \sum_{\chi \in \widehat{G}} \widehat{b}(\chi) \chi$. Thus \begin{equation}
		\langle a, b \rangle = \left(\frac{1}{|G|}\right)^2 \langle \widehat{a}(\chi), \widehat{b}(\chi) \rangle \delta_{\chi, \chi} = \frac{1}{|G|} \langle \widehat{a}, \widehat{b} \rangle.
	\end{equation}

\end{proof}

\begin{lemma}[Diaconis–Shahshahan Upper bound lemma]
	\label{lem:Upper bound lemma}
	Let $G$ be a finite abelian group, and let $F$ be the set of non-trivial irreducible characters of $G$. Let $Q$ be a probability on $G$.  Then:
	\begin{equation}
		\|Q - U\|^2_{1} \leq \sum_{\chi \in F} |\widehat{Q}(\chi)|^2.
	\end{equation}
\end{lemma}
\begin{proof}[Proof]

	and from \cref{lem:Absolute Value Inequality}, we have that:
	\begin{equation}
		 \| P - Q\|_1^2 \leq |G|^2 \| Q - U \|^2
	\end{equation}
	Then by \cref{thm:Plancherel},
	\begin{equation}
		|G|^2 \|Q - U \|^2 = |G| \left[ \langle \widehat{Q}, \widehat{Q} \rangle - 2 \langle \widehat{Q}, \widehat{U} \rangle + \langle \widehat{U}, \widehat{U} \rangle \right]
	\end{equation}
	As $\widehat{U} = \delta_{\chi_1}$, then $\widehat{U}(\chi) = \langle \chi_1, \chi \rangle$. 
	Therefore, $\langle \chi_1, \chi_1 \rangle = \frac{1}{|G|}$, and $\langle \widehat{Q}, \widehat{U} = \widehat{Q}(\chi_1) = 1/|G|$, and
	\begin{equation}
		\langle \widehat{Q}, \widehat{Q} \rangle =\frac{1}{|G|} + \frac{1}{|G|} \sum_{\chi \in F} \widehat{Q}(\chi) \overline{\widehat{Q}(\chi)}.
	\end{equation}
	Therefore, $|G|^2 \|Q - U \|^2 =  \sum_{\chi \in F}| \widehat{Q}(\chi)|^2$ and thus 
	\begin{equation}
		\|Q - U\|^2_{1} \leq \sum_{\chi \in F} |\widehat{Q}(\chi)|^2.
	\end{equation}
\end{proof}

\begin{corollary}
	\label{cor:Convolution Inequality}
	Let $G$ be a finite abelian group and let $P$ be a probability on $G$. Then 
	\begin{equation}
	\|P^{\ast k} - U\|^2_{1} \leq \sum_{\chi \in F} |\widehat{P}(\chi)|^{2k}.
\end{equation}
\end{corollary}

\begin{theorem}
	If $P$ is a probability on a finite abelian group $G$ and $P$ is ergodic, then $P^{\ast n}$ converges to $U$.
\end{theorem}
It suffices to show that $|\widehat{P}(\chi)| < 1$ for all nontrivial characters $\chi$ of $G$. Assume that $P(g) > 0$ with ergodicity. Recall that $|\widehat{P}(\chi)| = |\sum_{g \in G} P(G) \overline{\chi(g)}| \leq \sum_{g \in G} P(G) |\overline{\chi(g)}| = 1$ But we have that if $|\widehat{P}(\chi)| = 1$, then $P(g) \overline{\chi(g)}$ are non-negative multiples of a common complex number. But $P(1) \geq 0$ and $\overline{\chi(1)} = 1$. Therefore, $P(1) \overline{\chi(1)} = P(1)$, but we have that there is a $\overline{\chi(g)} \neq 1$ for some $g \in G$. Thus we have that equality does not hold, so $|\widehat{P}(\chi)| < 1$. 

\subsection{Discussion}
This result means that all random ergodic walks on groups that respect the group operation converge to a uniform distribution. Note that we have for any general Markov chain, then aperiodic and irreducible walks converge to some distribution, rather than a uniform distribution. This assumption is much weaker than on any set as we do not need aperiodicity to show convergence, but we do need the walk to respect the fact that groups are transitive. We use the fact that the group is finite abelian because the characters are very well defined: they are degree 1 and we can take tensor products to make characters for every element of the group in the natural way. 

In fact, there are similar results for groups that are nonabelian. We can drop some of the assumptions and have similar results for non-abelian groups.

\section{Examples}
\subsection{Ehrenfest's urn}
Suppose we have two urns, labelled $A$ and $B$, and urn $A$ contains $n$ balls and $B$ contains no balls. Then at each step in time, we choose a random ball and move it to the other urn. This is the same as acting on $(\mathbb{Z}_2)^n$ such that the states that you can reach from $g \in (\mathbb{Z}_2)^n$ are to flip one of the bits with uniform probability. 

\begin{theorem}
	Let $c > 0$ be a positive constant. Then there exists a sufficiently large $k$ such that:
	\begin{equation}
		\|P^{\ast k} - U\|^2_{TV} \leq \frac{1}{2} (e^{e^{-c}} - 1)
	\end{equation}
\end{theorem}

\subsection{Card shuffling}
Shuffling $n$ cards with a riffle-shuffle has a sharp cutoff at $\frac{3}{2} n \log n$ shuffles. 
\section{Conclusion}
We first discussed what a Markov chain is and what a stationary distribution looks like on a Markov chain. We then looked at what a random walk on a group is and how it connected to Markov chains. We also looked at how we could test if two Markov chains are close to each other. We then looked specifically at the case where the group is a finite abelian group. Abelian groups of characters are very easy to compute if you know the basis elements. We used Fourier analysis to prove a series of results on abelian groups of characters, and eventually showed an upper bound of the of the distance between a limiting ergodic distribution and the uniform distribution can get arbitrarily small, thus showing that any ergodic distribution converges to the uniform distribution. This result is much more helpful than the stationary distribution because it required less axioms (distribution does not have to be aperiodic) yet converged to a particular stationary distribution, the uniform distribution. We finally finished by looking at some examples. 

\printbibliography
\end{document}
