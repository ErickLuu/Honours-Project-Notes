\documentclass[]{article}
\usepackage[margin=1in]{geometry}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{url}
\usepackage{todonotes}
\usepackage{hyperref}
\usepackage[noabbrev, capitalise]{cleveref}

\usepackage[style = numeric,
isbn=false,
url=false,
eprint = false,
maxbibnames=99
]{biblatex}
\renewbibmacro{in:}{}
\DeclareSourcemap{
	\maps[datatype=bibtex]{
		\map{
			\step[fieldset=url, null]
			\step[fieldset=extra, null]
			\step[fieldset=urldate, null]
		}
	}
}
\AtEveryBibitem{%
	\clearfield{day}%
	\clearfield{month}%
	\clearfield{endday}%
	\clearfield{endmonth}%
}
\addbibresource{library.bib}
% Environments

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{conjecture}[theorem]{Conjecture}

\theoremstyle{definition}
\newtheorem{example}[theorem]{Example}

\numberwithin{theorem}{section}
\numberwithin{equation}{section}

\newcommand{\supp}{\text{Supp}}

%opening
\title{Random walks on groups report}
\author{Eric Luu}

\begin{document}

\maketitle
\section{Introduction}
Random walks on groups was originally motivated by the art of shuffling cards, popularised by mathemagician Persi Diaconis. One of the seminal results in random walks is the investigation into riffle shuffles, one of the most popular shuffling methods, by Bayer and Diaconis, \cite{bayerTrailingDovetailShuffle1992}. They found that the number of shuffles needed is at most $\frac{3}{2} \log n$ shuffles. Much of the work on random walks and groups have to do with the cutoff, or the time where the distribution is "close enough" to a uniform distribution. This is useful for shuffling cards, because we want that the cards that we shuffle to be uniform in some way.
\subsection{Outline of report}
We will outline the report as such.
\begin{enumerate}
	\item In \cref{sec:random walks}, we investigate what a random walk is and some important notions involving random walks. 
	\item In \cref{sec:defns}, we define some important terms specifically for random walks on groups. 
	\item In \cref{sec:Fourier Analysis}, we discuss the usage of Fourier analysis on finite abelian groups to yield results in the Fourier series.
	\item In \cref{sec:spectral theory}, we use the Fourier analysis before to build a spectral analysis of the random walks and we eventually show that the most important eigenvalue that appears is the uniform distribution. 
	\item In \cref{Examples}, we build on the theory developed on some example random walks. 
\end{enumerate}
Much of the literature is focused on building bounds of the rate of convergence of some shuffles. There is a lot of representation theory but much of the work of establishing bounds is combinatorial in nature. We do not focus on the combinatorial aspects as much in the report, though we do have a short discussion on using combinatorics in the examples page. 
\section{Random walks}\label{sec:random walks}

\subsection{Random walks}
We define a Markov chain on a set $\mathcal{X}$ in the following manner. We have a fixed probability distribution at each fixed point in $x$, so $P_x : \mathcal{X} \rightarrow [0, 1]$ is a \textit{probability distribution}. When $\mathcal{X}$ is finite, this means that the sum of $P_x$ over all of $\mathcal{X}$ is 1. 

We say a sequence of random variables $(X_0, X_1, ...)$ is a \textit{Markov chain} with\textit{ state space} $\mathcal{X}$ and \textit{distribution} $P$ if 
\begin{equation}
	P_x(y) = P(X_{t + 1} = y | X_t = x)
\end{equation}.
Essentially, this means that $P$ ignores all information about the previous steps except for the last. 

We will denote the Markov probability as $P$ where $P(x, y) = P_x(y)$. Typically $P$ is represented as a matrix where $[P]_{i,j} = P(i, j)$. 

\subsubsection{Definitions}
In standard Markov chain language, we say that a Markov chain is \textit{irreducible} if we can reach one state from another. A state $i$ has \textit{period} $k$ if $k$ is the gcd of all times that you can take to go back to $i$ if you were at $i$ at time $t = 0$. If $k = 1$, then we say $i$ is \textit{aperiodic}. 

A state $i$ is \textit{transient} if there is a nonzero probability that we will never reach $i$, and \textit{recurrent} otherwise. A state $i$ is \textit{positive recurrent} if the expected time to reach $i$ is finite. On a finite set $X$, recurrent implies positive recurrent. 

We say a Markov chain is \textit{ergodic} if every state is \textit{positive recurrent}. 
\subsubsection{Stationary distributions}
We wish to see what the distribution converges to in the long-run. We define a\textit{ stationary distribution} $\pi$ to be a vector on a transition matrix $P$ such that $\pi P = \pi$. 

\begin{theorem}
	If a Markov chain is irreducible and aperiodic, then there exists a unique stationary distribution $\pi$, and $P^{*k}$ converges to a rank one matrix where every row is $\pi$. 
\end{theorem}

\section{Random walks on groups}\label{sec:defns}
In this section, we will outline some definitions of Markov chains and walks. This section is based on Steinberg's chapter on random walks\cite{steinbergProbabilityRandomWalks2012}.

Suppose $G$ is a finite group and $X$ is a r.v. on $G$. $P$ is a probability distribution function where $P(g) = \text{Prob}[X = g]$ and $\sum_{g\in G} P(g) = 1$. We can treat this as a Markov chain but to go from $g$ to $gh$ is the same probability as going from $1$ to $h$ for all $g$ and all $h$. 

For a set $A \subseteq G$, we have that $P(A) = \sum_{g \in A} P(g)$. We define the support of $P$ as $\supp(P) = \lbrace g \in G : P(g) \neq 0 \rbrace$. 


\begin{example}
	These are examples of a random distribution on a group. 
	\begin{itemize}
		\item We define the \textit{uniform distribution} $U$ on $G$ as $U(g) = 1/|G|$ for all $g \in G$. 
		\item We define the subset distribution $S \subset G$ to be $1/|S|$ for all $ g \in S$ and 0 else.
	\end{itemize}
\end{example}

Instead of treating the group as a set, we can treat the elements as functions, specifically the left regular group action. Then we can act on the group and walk around it. The \textit{convolution} operation encapsulates taking steps around a group.
Let $P$, $Q$ be probabilities on $G$. Suppose we choose $X$ according to $P$ and $Y$ according to $Q$. Then the probability that $XY = g$ is the probability that $Y = h$ for some $h \in G$ and the probability that $X = gh^{-1}$. 
So $P \ast Q(g) := \text{Prob}[XY = g] = \sum_{h\in G} P(gh^{-1})Q(h) $.

\begin{proposition}
	Let $P$ and $Q$ be probability distributions on $G$. Then $P \ast Q$ is a probability distribution on $G$, with $\supp(P \ast Q) = \supp(P)\supp(Q) = \lbrace xy : x \in \supp(P), y \in \supp Q \rbrace$. 
\end{proposition}
\begin{proof}
	Trivial to show is between 0 and 1.
	Next, we have that:
	\begin{align*}
		\sum_{g \in G} P \ast Q(g) &= \sum_{g \in G} \sum_{h \in G} P(gh^{-1}) Q(h)\\
		&=\sum_{h \in G} Q(h) \sum_{g \in G} P(gh^{-1})\\
		&= \sum_{h \in G} Q(h)\\
		&= 1
	\end{align*}
\end{proof}
Thus this is a probability distribution. Obviously, the support is of the form $\left\{ xy : x \in \supp(P), y \in \supp(Q) \right\}$. 

The way to think about random walks is as follows. We start at the identity and move to element $x_1$ according to $P$. Then we choose another element $x_2$ according to $P$ and move to $x_1 x_2$. So this is a sequence of i.i.d. variables with distribution $P$.

This forms a Markov chain, and is an example of a random walk.

\begin{example}
	Examples of random walks on groups:
	\begin{itemize}
		\item Let $G$ be a group and $R$ be the generators and their inverses. Let $\Gamma$ be the Cayley graph of $G$. The simple random walk is to choose a random element $s \in R$ with uniform distribution and move to the vertex $gs$. By construction, the vertices adjacent to $\Gamma$ on $g$ are $gs$ with $s \in R$.
		\item Let $G = \mathbb{Z}_n$. Let us walk  $p, q \geq 0$, $p + q = 1$. Suppose we have that $P[X = 1] = p$, $P[x = -1] = q$ Then we go one step clockwise with probability $p$ and one step back with probability $q$. Then the probability distribution is $p \delta_1 + q \delta_{-1}$. 
		\item Visit every element on the group uniformly. 
	\end{itemize}
\end{example}


\subsection{Ergodicity}
We denote $P^{\ast k}$ as $\underbrace{P \ast P \ast ... \ast P}_{k \text{ times}}$. 
A random walk on a group $G$ driven by $P$ is said to be \textit{ergodic} if there exists an integer $N > 0$ such that $P^{\ast n}(g) > 0$ for all $n > N$ and $g \in G$. This means that after some point in time, the probability of reaching all states is nonzero after a certain burn-in period.

\subsection{Results for finite groups}
For probability distributions on groups, we have some stronger statements. In fact, we will show that the stationary distribution $\pi$ is uniform.
\begin{proposition}
	Let $P$ be a probability on a finite group $G$ and suppose $P(1) > 0$ and $Supp(P)$ generates the group $G$. Then the random walk driven by $P$ is ergodic.
\end{proposition}

If a walk has nonzero probability of reaching all states at time $N$ and it maintains this property on times $N+ 1, N + 2 , ..., 2N$, then the walk is ergodic all times after by the associative property of the convolution operation. This is an easy way to check if a walk is ergodic.

\begin{proof}
	Let $S = Supp(P)$. As $S$ is a generating set, then there exists an $N$ such that $S^N = G$. So we have that $P^{\ast N} g > 0$ by the definition of the support. Finally, $S^{k} \subseteq S^{k + 1}$ as $1 \in S$. Therefore, $P$ is ergodic. 
\end{proof}


\subsection{Notion of difference between probabilities}
We want to quantify how far a distribution is away from being uniform. We define the $L^1$ norm as $\|f\|_1 = \sum_{g \in G} |f(g)|$ for $f : G \rightarrow \mathbb{C}$. This forms a metric of functions on groups. 
We define the total variation distance between probabilities $P$ and $Q$ on a group $G$ as $\| P - Q\|_{TV} = \sup_{A \subseteq G} |P(A) - Q(A)|$. This is also referred to as the Gilbert-Shannon-Reeds metric. \cite{diaconisGeneratingRandomPermutation1981} 

\begin{lemma}
	Let $P$ and $Q$ be probabilities on $G$, and let $B = \lbrace g \in G : P(g) \geq Q(g) \rbrace$, $C = \lbrace g \in G : Q(g) \geq P(g) \rbrace$.
	Then $\|P - Q \|_{TV} = P(B) - Q(B) = Q(C) - P(C)$.
\end{lemma}
\begin{proof}
	We have that as $\sum_{g \in G} P(g) = \sum_{g \in G} Q(g) = 1$. 
	
	
	By definition, 
	\begin{equation}*
		\|P - Q\|_{TV} \geq |P(B) - Q(B)| = P(B) - Q(B).
	\end{equation}
	Let $A$ be a set such that $\|P - Q\|_{TV} = |P(A) - Q(A)|.$ Then $|P(A^c) - Q(A^c)| = |1 - P(A) - (1 - Q(A))| = |Q(A) - P(A)| = \|P - Q \|_{TV}$. Therefore, this is closed under complement. It is trivial to show that $P(B) - Q(B) = |Q(A) - P(A)|$. 
\end{proof}
Finally, we have that $\|P - Q \|_{TV}$ is a metric:
\begin{proposition}
	\label{prop:distribution difference equality}
	$\|P - Q\|_{TV} = \frac{1}{2} \|P - Q \|_1$. 
\end{proposition}
\begin{proof}
	\begin{align*}
		\|P - Q\|_{TV} &= \frac{1}{2}[P(B) - Q(B) + Q(C) - P(C)]\\
		&=\frac{1}{2}\left[ \sum_{g \in G : P(g) \geq Q(g)} (P(g) - Q(g)) + \sum_{g \in G : Q(g) \geq P(g)} (Q(g) - P(g)) \right]\\
		&= \frac{1}{2}\left[ \sum_{g \in G}|P(g) - Q(g)| \right]\\
		&= \frac{1}{2}\|P - Q\|_1
	\end{align*}
\end{proof}
Total variation is more commonly used in the literature.

We say that $P$ converges to $Q$ if for all $\varepsilon > 0$, there exists $N > 0$ such that $\|P^{*n}- Q \|_{TV} < \varepsilon$ when $n \geq N$. 

\section{Fourier analysis on finite groups}\label{sec:Fourier Analysis}
In this section, we will prove some important ideas about Fourier analysis and finite groups. We will mainly be using Steinberg \cite{steinbergProbabilityRandomWalks2012} for finite abelian groups, but we will be using a paper from Diaconis and Shahshahani's paper on random groups \cite{diaconisGeneratingRandomPermutation1981} for some results on symmetric groups. Below is an example from their paper \cite{diaconisGeneratingRandomPermutation1981}.

Let $G$ be a group. Let $\hat{G}$ be a the set of irreducible characters on $G$.
We have the general definition of Fourier transform. Let $f \in \mathbb{C}G$. Then $\widehat{f}: \widehat{G} \rightarrow \mathbb{C}$ is defined as:
\begin{equation}
	\widehat{f}(\chi) = |G| \langle f, \chi \rangle = \sum_{g \in G} f(g) \overline{\chi(g)}
\end{equation}

The Fourier inverse is defined as:

\begin{equation}
	f(x) = \frac{1}{|G|} \sum_{\chi \in \widehat{G}} \widehat{f}(\chi) \chi(x)
\end{equation}
For some results, we will work over abelian groups. 
Abelian groups are nice as all irreducible characters are degree 1 characters and also representations. Also, the number of characters is the number of elements in $G$. Finally, $ZL(G) = \mathbb{C}(G)$ for abelian groups. 

Furthermore, we have that if $\chi, \psi \in \widehat{G}$, then $\widehat{\chi}(\psi) = \langle \chi, \psi \rangle = |G|\delta_{\chi, \psi}$. So $\widehat{\chi} = |G| \delta_{\chi}$, where $\delta_{\chi}(\psi) = 1$ if $\psi = \chi$ and 0 else. 
So the duals of irreducible characters work like functionals on $\widehat{G}$> 
Now we will show that taking dual groups transforms convolutions into something nicer, the product. We prefer to work over multiplication because it is a much easier operation to deal with. 

\begin{theorem}
	\label{thm:abelian dual commutation}
	Let $G$ be an group, and $a, b \in \mathbb{C}(G)$. Then $ \widehat{a \ast b} = \widehat{a} \widehat{b}$. 
\end{theorem}

\begin{proof}
	We have that:
	\begin{align*}
		\widehat{a \ast b}(\chi) &= |G| \langle a \ast b, \chi \rangle\\
		&= \sum_{g \in G} (a \ast b) (g) \overline{\chi(g)} \\
		&= \sum_{g \in G} \overline{\chi(g)} 
		\sum_{h \in G} a(g h^{-1}) b(h) \\
		&= \sum_{h \in G} b(h)  
		\sum_{g \in G} a(g h^{-1})\overline{\chi(g)}  
	\end{align*} 
	And setting $k = gh^{-1}$, so $g =kh$, we get that:
	\begin{align*}
		&= \sum_{h \in G} b(h)  
		\sum_{k \in G} a(k)\overline{\chi(kh)} \\
		&= \sum_{h \in G} b(h)  
		\sum_{k \in G} a(k)\overline{\chi(k)} \overline{\chi(h)} \\
		&=  \sum_{h \in G} b(h)  \overline{\chi(h)} 
		\sum_{k \in G} a(k)\overline{\chi(k)}\\
		&= \widehat{a}(\chi) \widehat{b}(\chi)
	\end{align*}
\end{proof}

We have a result involving class functions that will be useful when we are working with any group. 
\begin{theorem}\label{thm:class_fns}
	Let $G$ be a finite group and $\rho$ an irrep of $G$. let $P$ be a class function and let $C = \lbrace C_1... C_n \rbrace$ be the conjugacy classes with $\lbrace c_1, ..., c_n \rbrace$ the associated representatives in $G$, and suppose $\lbrace C_1, ..., C_n \rbrace$ is real. Then:
	\begin{equation}
		\widehat{P}(\rho)= C \times Id \qquad \text{where} \qquad C = \frac{1}{\deg(\rho)}  \sum_{i = 1}^n\left(|C_i|\right) P(c_i) \chi_\rho(c_i).
	\end{equation}
\end{theorem}
\begin{proof}
	We will use Schur's lemma to prove this result. 
	Let $M_i = \sum_{c \in C_i} \rho(c)$. Then we have that:
	\begin{equation}
			\widehat{P}(\rho) = \sum_{g \in G} P(g) \overline{\rho(g)} = \sum_{g \in G} P(g) \rho(g) =  \sum_{i = 1}^n P(c_i) M_i
	\end{equation}
	We have that $\rho(g) \left(\sum_{c \in C_i} \rho(c)\right) \rho(g^{-1}) = M_i$ for all $g \in G$. Therefore, we use Schur's lemma to say that $M_i = K_i I$ for some real number $K_i$. Then by taking the trace, we have that:
	\begin{equation}
		Tr(M_i) = |C_i| \chi_\rho(c_i) = K_i \deg(\rho).
	\end{equation}
	Thus shown. 
\end{proof}
We will finish with an example by Diaconis and Shahshahani. This example involves swapping two cards in the deck with uniform probability. There is also a probability that you do nothing. \cite{diaconisGeneratingRandomPermutation1981}
\begin{example}\label{ex:swap two cards}
	Let $T$ be a probability measure on $S_n$ defined as such:
	\begin{equation}
		T(\sigma) = \begin{cases}
			\frac{1}{n} , &\sigma = 1\\
			\frac{2}{n^2} , &\sigma = \text{ transposition}\\
			0 , &\text{else}
		\end{cases}
	\end{equation}
	Then $T$ is a class function, and from \cref{thm:class_fns}, we have that for $\rho \in S_n$:
	\begin{equation}
		\widehat{T}(\rho) = \left(\frac{1}{n} + \frac{n-1}{n} \frac{\chi_\rho(\tau)}{\deg(\rho)}\right) Id.
	\end{equation}
\end{example}
We have that $\chi(1) = \deg(\rho)$, so this is the identity permutation. Next, we have $|C_{\text{transpositions}}| = \binom{n}{2} = \frac{n(n-1)}{2}$, so we have the result above.
\subsection{Dual groups of abelian groups}
Let $G$ be a finite group and let $\widehat{G}$ be the set of irreducible characters $\chi: G \rightarrow \mathbb{C}$. If $G$ is also abelian, then we have a stronger result. 
\begin{theorem}
	If $G$ is abelian, then $\widehat{G}$ is a group where multiplication is defined as $(\chi_{ab})(g) = \chi_a(g)\chi_b(g)$, inverses are defined as $\chi_a^{-1} = \overline{\chi_a}$, and $\chi_1$, the trivial group, is the identity. 
\end{theorem}
\begin{proof}
	We have that $\widehat{G}$ are characters of degree 1.
	 We have that for $\chi_a \in \widehat{G}$, $\chi_a(g)$ is a root of unity of $|g|$. Then multiplying two characters also yields a root of unity. To show that the multiplication of two elements in $\widehat{G}$ yields another character, recall that $\rho_{ab} = \rho_a \otimes \rho_b$ implies that $\chi_{ab} = \chi_a \times \chi_b$. But as these characters are representations of degree 1, then multiplying elements is the same as taking the tensor products of their respective representations. But since the tensor products are also of degree 1, then they must also be characters in $\widehat{G}$. 
\end{proof}
What is the most important part about $G$ being abelian is that the representations of $G$ are all of degree $1$, so characters are representations. 
\begin{theorem}
	Let $G$ be a finite abelian group. Then $G \cong \widehat{G}$.
\end{theorem}
\begin{proof}
	This is easy to see for cyclic groups $\mathbb{Z}_n$, as for any generator $\langle k \rangle$, we send $\rho_a(k) = e^{2 i \pi a / n}$. Now for any finite cyclic group $\mathbb{Z}_{\alpha} \times \mathbb{Z}_{\beta}... $ (we can do this using the characterisation of abelian groups) with generators $\langle 1_{\alpha}, 1_\beta... \rangle$, we look at the irreducible representations on just $\mathbb{Z}_{i}$ and take the tensor product with all the other representations on $\mathbb{Z}_j$ for any different $j$ in the group product to form the character group. 
\end{proof}

\begin{theorem}
	\label{thm:Eigenvector operators}
	Let $G$ be a finite abelian group and $a \in \mathbb{C}(G)$. Define the operator $A: \mathbb{C}(G) \rightarrow \mathbb{C}(G)$ by $A(b) = a \ast b$. Then $A$ is linear and furthermore $\chi$ is an eigenvector with eigenvalue $\widehat{a}(\chi)$ for all $\chi \in \widehat{G}$. Therefore, $A$ is diagonalisable. 
\end{theorem}

\begin{proof}
	First, we have that $\widehat{a\ast \chi} = \widehat{a} \widehat{\chi} = \widehat{a} |G| \delta_\chi$, and $\widehat{a} |G| \delta_\chi(\psi) = \widehat{a}(\psi) |G|$ if $\psi = \chi$ and 0 else, so $\widehat{a} |G| \delta_\chi = \widehat{a}(\chi) |G| \delta_\chi$. When we apply the inverse of the Fourier transform, we have that:
	\begin{align*}
		a \ast \chi &= \frac{1}{|G|}\sum_{\psi \in \widehat{G}} \widehat{a \ast \chi}(\psi) \psi\\
		&=
		\frac{1}{|G|}\sum_{\chi \in \widehat{G}} \widehat{a}(\psi) |G| \widehat{\chi}(\psi) \psi\\
		&= \widehat{a}(\chi) \chi
	\end{align*}
	from the first orthogonality. So $A \chi = \widehat{a}(\chi) \chi$. Thus shown. 
\end{proof}

As an example of $\widehat{a}$, we have that for $U$, the uniform distribution, we have that $\widehat{U}(\chi) = |G|\langle U, \chi \rangle = \langle \chi_1, \chi \rangle $. So $\widehat{U} = \delta_{\chi_1}$. 

\section{Spectrums of random walks on finite abelian groups}\label{sec:spectral theory}
In this section, we apply spectral theory to convolutions of distribions on groups to see what eigenvalues show up, where the eigenvectors are distributions on groups. We wish to show that the convolutions that come out of the eigenvectors have eigenvalues whose absolute values are strictly less than one, except for the uniform distribution, which has eigenvalue 1.

Define a convolution operator on a probability distribution $P$ as $M : \mathbb{C}G \rightarrow \mathbb{C}G$ where $M(a) = P \ast a$, where $(P \ast a) (g) =\sum_{h\in G} P(gh^{-1})a(h)$.
\begin{definition}
	The spectrum of a random walk driven by probability distribution $P$ is the set of eigenvalues of $M$. 
\end{definition} 
It is easy to show that $P \ast U = U$ and that $U$ is an eigenvector of $M$ with eigenvalue $1$. 


\begin{theorem}[Diaconis]
	\label{thm:Diaconis}
	Let $G$ be a finite abelian group and let $P$ be a probability on $G$. Then $spec(P) := \lbrace \widehat{P}(\chi) : \chi \in \widehat{G} \rbrace$ where the multiplicity of $\lambda \in spec(P)$ are the number of characters where $\widehat{P}(\chi) = \lambda$. The orthonormal basis for the eigenspace of $\lambda$ is the set of all characters $\chi$ where $\widehat{P}(\chi) = \lambda$. 
\end{theorem}
This is a special case of \cref{thm:Eigenvector operators}.

\begin{lemma}
	\label{lem:Absolute Value Inequality}
	Let $G$ be a finite group. Define $\|f\| = \sqrt{\langle f, f \rangle}$, where $f \in \mathbb{C}(G)$. Then $\|f \|_1 \leq |G| \|f \|$.
\end{lemma}
\begin{proof}
	This comes from the Cauchy-Schwartz inequality. We have that \begin{equation}
		\| f \|_1^2 = \left(\sum_{g \in G} |f(g)|\right)^2 \leq \sum_{g \in G} |f(g)|^2 = |G| \| f\|.
	\end{equation}
\end{proof}
This result by Plancharel will be used to prove an important upper bound. 

\begin{theorem}[Plancherel]
	\label{thm:Plancherel}
	Let $G$ be a finite group and let $a, b \in \mathbb{C}(G)$. Then $\langle a, b \rangle = \frac{1}{|G|} \langle \widehat{a}, \widehat{b} \rangle$. 
\end{theorem}
Recall that $\langle \widehat{a}, \widehat{b} \rangle = \frac{1}{|\widehat{G}|} \sum_{\chi \in \widehat{G}} \widehat{a}(\chi) \overline{\widehat{b}(\chi)}$. 
\begin{proof}
	We have that $a =\frac{1}{|G|} \sum_{\chi \in \widehat{G}} \widehat{a}(\chi) \chi$, and $b = \frac{1}{|G|} \sum_{\chi \in \widehat{G}} \widehat{b}(\chi) \chi$. By the first orthogonality relation, the inner product of irreducible characters $\chi$ and $\psi$ is $1$ if $\chi = \psi$ and 0 else.
	
	\begin{equation}
		\langle a, b \rangle = \left(\frac{1}{|G|}\right)^2 \langle \widehat{a}(\chi), \widehat{b}(\chi) \rangle \delta_{\chi, \chi} = \frac{1}{|G|} \langle \widehat{a}, \widehat{b} \rangle.
	\end{equation}

\end{proof}
We have another result by Plancherel, which we lift from Serre \cite{serreExamples1977}. This example is used to bound above the rate of convergence for any group. We will not discuss the proof in the report.
\begin{theorem}[Plancherel]\label{lem:Plancharel formula}
	If $P$ is a function on $G$, then
	\begin{equation}
		\sum_{g \in G}|P(g)|^2 = \frac{1}{|G|} \sum_{\rho \in \widehat{G}} \deg(\rho) Tr(\widehat{P}(\rho) \overline{\widehat{P}(\rho)}) 
	\end{equation}
\end{theorem}
For abelian groups, we can use the Diaconis- Shahshahani Upper Bound Lemma to bound above the rate of convergence of a group. This is generally a stronger bound than Plancharel's inequality. 
\begin{lemma}[Diaconisâ€“Shahshahani Upper Bound Lemma]
	\label{lem:Upper bound lemma}
	Let $G$ be a finite abelian group, and let $F$ be the set of non-trivial irreducible characters of $G$. Let $Q$ be a probability on $G$.  Then:
	\begin{equation}
		\|Q - U\|^2_{1} \leq \sum_{\chi \in F} |\widehat{Q}(\chi)|^2.
	\end{equation}
\end{lemma}
\begin{proof}[Proof]

	and from \cref{lem:Absolute Value Inequality}, we have that:
	\begin{equation}
		 \| Q - U\|_1^2 \leq |G|^2 \| Q - U \|^2
	\end{equation}
	Then by \cref{thm:Plancherel},
	\begin{equation}
		|G|^2 \|Q - U \|^2 = |G| \left[ \langle \widehat{Q}, \widehat{Q} \rangle - 2 \langle \widehat{Q}, \widehat{U} \rangle + \langle \widehat{U}, \widehat{U} \rangle \right]
	\end{equation}
	As $\widehat{U}$ is $1$ when we put in the trivial character and $0$ everywhere else, then $\langle \widehat{U}, \widehat{U} \rangle = \frac{1}{|G|}$. $\langle \widehat{Q}, \widehat{U} \rangle = \widehat{Q}(\chi_1) = 1/|G|$, and
	\begin{equation}
		\langle \widehat{Q}, \widehat{Q} \rangle =\frac{1}{|G|} + \frac{1}{|G|} \sum_{\chi \in F} \widehat{Q}(\chi) \overline{\widehat{Q}(\chi)}.
	\end{equation}
	Therefore, $|G|^2 \|Q - U \|^2 =  \sum_{\chi \in F}| \widehat{Q}(\chi)|^2$ and thus 
	\begin{equation}
		\|Q - U\|^2_{1} \leq \sum_{\chi \in F} |\widehat{Q}(\chi)|^2.
	\end{equation}
\end{proof}

As a corollary, we can show that we can have an upper bound of the total variation. 
\begin{corollary}
	Let $G$ be a finite abelian group, and let $F$ be the set of non-trivial irreducible characters of $G$. Let $Q$ be a probability on $G$.  Then:
\begin{equation}
	\|Q - U\|^2_{TV} \leq \frac{1}{4}\sum_{\chi \in F} |\widehat{Q}(\chi)|^2.
\end{equation}
\end{corollary}
This is also referred to as the Upper Bound Lemma in the literature.

\begin{corollary}
	\label{cor:Convolution Inequality}
	Let $G$ be a finite abelian group and let $P$ be a probability on $G$. Then 
	\begin{equation}
	\|P^{\ast k} - U\|^2_{1} \leq \sum_{\chi \in F} |\widehat{P}(\chi)|^{2k}.
\end{equation}
\end{corollary}

\begin{theorem}
	If $P$ is a probability on a finite abelian group $G$ and $P$ is ergodic, then $P^{\ast n}$ converges to $U$.
\end{theorem}
It suffices to show that $|\widehat{P}(\chi)| < 1$ for all nontrivial characters $\chi$ of $G$. Assume that $P(g) > 0$ with ergodicity. Recall that $|\widehat{P}(\chi)| = |\sum_{g \in G} P(G) \overline{\chi(g)}| \leq \sum_{g \in G} P(G) |\overline{\chi(g)}| = 1$. But we have that if $|\widehat{P}(\chi)| = 1$, then $P(g) \overline{\chi(g)}$ are non-negative multiples of a common complex number. But $P(1) \geq 0$ and $\overline{\chi(1)} = 1$. Therefore, $P(1) \overline{\chi(1)} = P(1)$, but we have that there is a $\overline{\chi(g)} \neq 1$ for some $g \in G$. Thus we have that equality does not hold, so $|\widehat{P}(\chi)| < 1$. 

\subsection{Discussion}
This result means that all random ergodic walks on groups that respect the group operation converge to a uniform distribution. Note that we have for any general Markov chain, then aperiodic and irreducible walks converge to some distribution, rather than a uniform distribution. This assumption is much weaker than on any set as we do not need aperiodicity to show convergence, but we do need the walk to respect the fact that groups are transitive. We use the fact that the group is finite abelian because the characters are very well defined: they are degree 1 and we can take tensor products to make characters for every element of the group in the natural way. 

In fact, there are similar results for groups that are nonabelian, but require additional conditions. 

\section{Examples}\label{Examples}

\subsection{Swapping two cards}
This proof comes from Diaconis and Shahshahani \cite{diaconisGeneratingRandomPermutation1981}. 
Let $T$ be defined as in \cref{ex:swap two cards}. Then we have that:
	\begin{lemma}
	\begin{equation}
		\| T^{\ast k} - U \|_1 \leq \left[\sum_\rho d^2_\rho \left(\frac{1}{n} + \frac{n-1}{n} \deg(\rho)\right)^{2k}\right]^{\frac{1}{2}}
	\end{equation}
\end{lemma}

\begin{proof}
	From Cauchy-Schwartz and the triangle inequality, we have that

\begin{equation}
	\| T^{\ast k} - U \|_1 = \sum_{g \in S_n} \left|T^{\ast k}(g) - \frac{1}{n!}\right| \leq \left[n! \sum_{g \in S_n} \left(T^{*k}(g) - \frac{1}{n!}\right)^2\right]^{\frac{1}{2}}
\end{equation}
From \cref{lem:Plancharel formula}, we have that
\begin{equation}
	n! \sum_{g \in S_n} \left(T^{*k}(g) - \frac{1}{n!}\right)^2 = \sum_\rho d_\rho Tr\left([\widehat{T}^{2k} - \widehat{U}] \rho^2\right)
\end{equation}

Then from \cref{ex:swap two cards}, we have that
\begin{equation}
	\widehat{T}(\rho)= \left(\frac{1}{n} + \frac{n-1}{n} \frac{\chi_\rho(\tau)}{\deg(\rho)}\right) Id.
\end{equation}
By \cref{thm:class_fns}, we have that:
\begin{equation}
	\widehat{T}^{\ast k}(\rho)= \left(\frac{1}{n} + \frac{n-1}{n} \frac{\chi_\rho(\tau)}{\deg(\rho)}\right)^k Id.
\end{equation}
where we have that $\rho = \overline{\rho}$ because we are working over the symmetric group.
We also have that $\widehat{U}(\rho) = 0$ for any nontrivial $\rho$. Therefore, we have that by additivity: 
\begin{equation}
	Tr((\widehat{T}^{\ast k}  - \widehat{U})(\rho)^2 ) = \left(\frac{1}{n} + \frac{n-1}{n} \frac{\chi_\rho(\tau)}{\deg(\rho)}\right)^{2k} Id.
\end{equation}

and the necessary result is given. 
\end{proof}

In the paper by Diaconis and Shahshahani\cite{diaconisGeneratingRandomPermutation1981}, they bound this constant above by looking at each cycle length and creating ``zones'' which partition the set of partitions of $n$. This proof is combinatorial in nature and will not be discussed more here. 
\subsection{Ehrenfest's urn}
Suppose we have two urns, labelled $A$ and $B$, and urn $A$ contains $n$ balls and $B$ contains no balls. Then at each step in time, we choose a random ball and move it to the other urn. In statistical mechanics, this is a very simple model of heat transfer or particle exchange. As an example, we can imagine this as a simplified diffusion model between two boxes. This is the same as acting on $(\mathbb{Z}_2)^n$ such that the states that you can reach from $g \in (\mathbb{Z}_2)^n$ are to flip one of the bits with uniform probability. 

The probability function is given by $P = \frac{1}{n} \left(\delta_{e_1} ... \delta_{e_n}\right)$

Here is an upper bound of how fast $P$ converges to $U$. 
\begin{theorem}
	Let $c > 0$ be a positive constant. Then there exists a sufficiently large $k$ such that:
	\begin{equation}
		\|P^{\ast k} - U\|^2_{TV} \leq \frac{1}{4} (e^{e^{-c}} - 1)
	\end{equation}
\end{theorem}
The proof comes from Steinberg in \cite{steinbergProbabilityRandomWalks2012}. 

\subsubsection{Proof}
Let $e_i$ be the state with all zeroes except for $1$ in the $i$-th column. We have that $P(e_i) = \frac{1}{n}$ for all $n$. 
Every character of $\left(\mathbb{Z}_2\right)^n$ is given by a set $Y \subset [n]$. Thereis also a function $\alpha: G \rightarrow 2^{[n]}$ which is defined as $\alpha(v) = \langle i : v_i = 1 \rangle$. Then the character with respect to $Y$ is:
\begin{equation}
	\chi_Y(v) = (-1)^{|\alpha(v) \cap Y|}
\end{equation}

Therefore, we have that $\widehat{P}(\chi_Y)$ has the elements $y \in Y$ contribute $ -1/n$ each to $\widehat{P}(\chi_Y)$. This is because $\widehat{P}(\chi_Y) = \sum_{1 \leq i \leq n} P(e_i) \chi_Y(e_i)$. By the same calculation, the elements not in $Y$ contribute $1/n$ to $\widehat{P}(\chi_Y)$. Taking the sums, we have that the spectrum must be $\frac{1}{n}(n - 2 |Y|) = 1 - \frac{2|Y|}{n}$. As there are $\binom{n}{j}$ subsets $Y$ where $|Y| = j$,
from \cref{lem:Upper bound lemma} we have that:

\begin{equation}
	\left\| P^{\ast k} - U \right\|_{TV}^2 \leq \frac{1}{4} \sum_{j = 1}^{n}\binom{n}{j} \left[1 - \frac{2j}{n}\right]^{2k}
\end{equation}

\begin{equation}
	\left\| P^{\ast k} - U \right\|_{TV}^2 \leq \frac{1}{4} \sum_{j = 1}^{n}\binom{n}{j} \left[1 - \frac{2j}{n}\right]^{2k}
\end{equation}
Now we have that $\binom{n}{j} \leq \frac{n^j}{j!}$ trivially and that $\left[1 - \frac{2j}{n}\right]^{2k} < e^{2k(- \frac{2j}{n})} = e^{-\frac{4jk}{n}}$ asymptotically. 
Therefore, 
\begin{equation}
	\left\| P^{\ast k} - U \right\|_{TV}^2 \leq \frac{1}{4} \sum_{j = 1}^{n}\frac{n^j}{j!} e^{-\frac{4jk}{n}}
\end{equation}

If we let $k \geq (n)(\log n + c)/4$, then we have that $e^{-\frac{4jk}{n}} \leq e^{j \log (n - jc)}= e^{- j c}/ n^j$.

Plugging this into the equation. we get that:
\begin{align*}
	\left\| P^{\ast k} - U \right\|_{TV}^2 &\leq \frac{1}{4} \sum_{j = 1}^{n}\frac{1}{j!}e^{-jc}\\	
	&\leq \frac{1}{4}\sum_{j = 1}^{\infty}\frac{1}{j!}\left(e^{-c}\right)^j\\
	&\leq \frac{1}{4}\left(e^{e^{-c}} - 1\right)
\end{align*}
This is a simple proof to show that we achieve convergence to a uniform bound with an upper limit on the rate of convergence. This proof is combinatorial in nature, except for the use of the upper bound lemma. However, it illustrates how combinatorics and Fourier analysis may be combined to prove an upper bound of the rate of convergence.

	\subsection{Riffle-shuffling of a deck of cards}This comes from Bayer and Diaconis \cite{bayerTrailingDovetailShuffle1992}. A riffle-shuffle is when the deck is split into two heaps and cards from each heap are shuffled through either heap with a certain probability. The model that was used was that cards were dropped from the left and right heaps with a probability proportional to the number of cards in each heap, so if there are $A$ and $B$ cards remaining in each heap, the probability of choosing a card from the left heap is $\frac{A}{A + B}$ and the probability of choosing a card from the right heap is $\frac{B}{A + B}$. Then given $n$ cards, the number of these operations that need to be performed is at most $\frac{3}{2} \log n$ to reach a cutoff point. A \textit{cutoff point} is a point where extra shuffles will not make the deck more random. For a deck of 52 cards, the number is around 7 shuffles. 
\section{Conclusion}
Investigating statistical or combinatorial problems such as card shuffling can lead to interesting mathematics. In particular, if the problem space is nice, like on a group, then we can use tools such as representation theory and Fourier analysis on groups to build good bounds on how fast we converge to the uniform distribution. We can apply this knowledge to statistical and stochastic models to build an understanding of the rate of convergence of a particular system. This report mainly focused on the representation theory part on this area of knowledge, though we do discuss the combinatorial aspects as well. 
\printbibliography
\end{document}
