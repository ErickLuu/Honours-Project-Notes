\documentclass[]{article}
\usepackage[margin=1in]{geometry}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{url}
\usepackage{hyperref}
\usepackage{cleveref}

% Environments

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{conjecture}[theorem]{Conjecture}

\theoremstyle{definition}
\newtheorem{example}[theorem]{Example}

\numberwithin{theorem}{section}
\numberwithin{equation}{section}

\newcommand{\supp}{\text{Supp}}

%opening
\title{Random walks on groups notes}
\author{Eric Luu}

\begin{document}

\maketitle
\section{Motivation}
Notes from Steinberg, Representation theory of finite groups, chapter 11. All group multiplication will be done left to right. 
In a famous paper " Trailing the dovetail shuffle to its lair", Bayer and Diaconis showed that you had to riffle-shuffle a deck of cards seven times to randomly shuffle the deck. The paper used random walks on $S_n$ to model shuffling the deck.

\section{Definitions}
Suppose $G$ is a finite group and $P$ is a probability distribution function where $P(g) = \text{Prob}[X = g]$ and $\sum_{g\in G} P(g) = 1$.

For a set $A \subseteq G$, we have that $P(A) = \sum_{g \in A} P(g)$. We define the support of $P$ as $\supp(P) = \lbrace g \in G : P(g) \neq 0 \rbrace$. 
\section{Creating probability distributions}
We define the uniform distribution $U$ on $G$ as $U(g) = 1/|G|$ for all $g \in G$. 
We can convolve a probability like so. Let $P$, $Q$ be probabilities on $G$. Suppose we choose $X$ according to $P$ and $Y$ according to $Q$. Then the probability that $XY = g$ is the probability that $Y = h$ for some $h \in G$ and the probability that $X = gh^{-1}$. 
So $P \ast Q(g) := \text{Prob}[XY = g] = \sum_{h\in G} P(gh^{-1})Q(h) $.

\begin{proposition}
	Let $P$ and $Q$ be probability distributions on $G$. Then $P \ast Q$ is a probability distribution on $G$, with $\supp(P \ast Q) = \supp(P)\supp(Q) = \lbrace xy : x \in \supp(P), y \in \supp Q \rbrace$. 
\end{proposition}
\begin{proof}
	Trivial to show is between 0 and 1.
	Next, we have that:
	\begin{align*}
		\sum_{g \in G} P \ast Q(g) &= \sum_{g \in G} \sum_{h \in G} P(gh^{-1}) Q(h)\\
		&=\sum_{h \in G} Q(h) \sum_{g \in G} P(gh^{-1})\\
		&= \sum_{h \in G} Q(h)\\
		&= 1
	\end{align*}
\end{proof}
Thus this is a probability distribution. Note that the support is also trivial.

\section{Notion of difference between probabilities}
We want to quantify how far a distribution is away from being uniform. We define the $L^1$ norm as $\|f\|_1 = \sum_{g \in G} |f(g)|$ for $f : G \rightarrow \mathbb{C}$. The $L^1$-norm is a metric and also $\|a \ast b\|_1 \leq \|a\|_1 \cdot \|b\|_1$. 

Proof in notes.

We define the total variation distance between probabilities $P$ and $Q$ on a group $G$ as $\| P - Q\|_{TV} = \sup_{A \subseteq G} |P(A) - Q(A)|$. 

\begin{lemma}
	Let $P$ and $Q$ be probabilities on $G$, and let $B = \lbrace g \in G : P(g) \geq Q(g) \rbrace$, $C = \lbrace g \in G : Q(g) \geq P(g) \rbrace$.
	Then $\|P - Q \|_{TV} = P(B) - Q(B) = Q(C) - P(C)$.
\end{lemma}
\begin{proof}[heading]
	We have that as $\sum_{g \in G} P(g) = \sum_{g \in G} Q(g) = 1$. 
	
	
	By definition, $\|P - Q\|_{TV} \geq |P(B) - Q(B)| = P(B) - Q(B)$.
	Let $A$ be a set such that $\|P - Q\|_{TV} = |P(A) - Q(A)|.$ Then $|P(A^c) - Q(A^c)| = |1 - P(A) - (1 - Q(A))| = |Q(A) - P(A)| = \|P - Q \|_{TV}$. Therefore, this is closed under complement. It is trivial to show that $P(B) - Q(B) = |Q(A) - P(A)|$. 
\end{proof}
Finally, we have that $\|P - Q \|_{TV}$ is a metric:
\begin{proposition}
	\label{prop:distribution difference equality}
	$\|P - Q\|_{TV} = \frac{1}{2} \|P - Q \|_1$. 
\end{proposition}

\begin{proof}
	\begin{align*}
		\|P - Q\|_{TV} &= \frac{1}{2}[P(B) - Q(B) + Q(C) - P(C)]\\
		&=\frac{1}{2}\left[ \sum_{g \in G : P(g) \geq Q(g)} (P(g) - Q(g)) + \sum_{g \in G : Q(g) \geq P(g)} (Q(g) - P(g)) \right]\\
		&= \frac{1}{2}\left[ \sum_{g \in G}|P(g) - Q(g)| \right]\\
		&= \|P - Q\|_1
	\end{align*}
\end{proof}

We say that a sequence of probabilities $\lbrace P_n \rbrace_{n \in \mathbb{N}}$ converges to $P$ if for all $\varepsilon > 0$, there exists $N > 0$ such that $\|P_n - P \|_{TV} < \varepsilon$ when $n \geq N$. 
\section{Random walks}
We denote $P^{\ast k}$ as $\underbrace{P \ast P \ast ... \ast P}_{k \text{times}}$. 
The way to think about random walks is as follows. We start at the identity and move to $X_1$ of $G$ according to $P$. Then we choose another element $X_2$ and move to $X_1 X_2$. So this is a sequence of i.i.d. variables with distribution $P$.

Let $Y_0$ be the random variable with distribution $\delta_1$, so $Y_0 = id$ with probability 1. Then $Y_k = Y_{k-1} X_k$ for $k \geq 1$. This $Y_k$ is the distribution of the position on the $k$-th step of the random walk. We have that $Y_k$ is a random variable with distribution $P^{\ast k}$. 

This forms a Markov chain. 
\subsection{Examples of random walks on graphs}
Let  $G$ be a group and $S$ be a symmetric subset. Let $\Gamma$ be the Cayley graph of $G$ with respect to $S$. The simple random walk is to choose a random element $s \in S$ with uniform distribution and move to the vertex $gs$. By construction, the vertices adjacent to $\Gamma$ on $g$ are $gs$ with $s \in S$.

Let $p, q \geq 0$, $p + q = 1$. Suppose we move around an $n$-gon. Then we go one step clockwise with probability $p$ and one step back with probability $q$. Then the probability distribution is $p \delta_1 + q \delta_{-1}$. 

\subsection{Ergodicity}
A random walk on a group $G$ driven by a probability $P$ is said to be ergodic if there exists an integer $N > 0$ such that $P^{\ast N}(g) > 0$ for all $g \in G$, so every event can be reached from $P$. 

\begin{proposition}
	Let $P$ be a probability on a finite group $G$ and suppose $P(1) > 0$ and $Supp(P)$ generates the group $G$. Then the random walk driven by $P$ is ergodic.
\end{proposition}

\begin{proof}
	Let $S = Supp(P)$. As $S$ is a generating set, then there exists an $N$ such that $S^N = G$. So we have that $P^{\ast N} g > 0$ by the definition of the support. Finally, $S^{k} \subseteq S^{k + 1}$ as $1 \in S$. Therefore, $P$ is ergodic. 
\end{proof}
\section{Proof of convergence}
In this section, we shall prove the theorem below.
\begin{theorem}
	If $P$ is an ergodic random walk on a finite group $G$, then $(P^{\ast k})$ converges to the uniform distribution $U$. 
\end{theorem}
Recall $\mathcal{C}(G)$ is the group algebra on $G$, where $\mathcal{C}(G) = \lbrace f : f: G \rightarrow \mathbb{C} \rbrace$. This is an inner product space where $\langle f_1, f_2 \rangle = 1/|G| \sum_{g \in G} f_1(g) \overline{f_2(g)}$. 

Recall that $\chi_\rho: G \rightarrow \mathbb{C}$ is a character by having that $\chi_\rho(g) = tr(\rho(g))$.

\section{Fourier analysis on finite groups}
\subsection{Fourier analysis on cyclic groups}
We have that all irreducible characters from $\mathbb{Z}_n \rightarrow \mathbb{C}$ are of the form $\chi_j$, $0 \leq j \leq n-1$, which maps $[a]$ to $e^{2 \pi i a j/n}$. 

Now we have functions $f: \mathbb{Z}_n \rightarrow \mathbb{C}$ are the periodic functions of period $n$ from $\mathbb{Z} \rightarrow \mathbb{C}$. We have that the characters ${\chi_j}_{1 \leq j \leq n-1}$ form a basis of $\mathbb{C}\mathbb{Z}_n$, so we have that $f : \mathbb{Z}_n \rightarrow \mathbb{C}$ can be written as $f = \langle f, \chi_0 \rangle \chi_0 + \langle f, \chi_1 \rangle \chi_1 + ... + \langle f , \chi_{n-1} \rangle \chi_{n+1}$.

We can define a Fourier transform $\widehat{f}: \mathbb{Z}_n \rightarrow \mathbb{C}$ by 
\begin{equation}
	\widehat{f}([m]) = n \langle f, \chi_m \rangle = \sum_{i = 0}^{n-1}f([k]) e^{-2\pi i ma/n}.
\end{equation}

This is invertible, so we have that
\begin{equation}
	f = \frac{1}{n}\sum_{j = 0}^{n-1} \widehat{f}([j])\chi_j
\end{equation}
.

\subsection{Dual groups}
Let $G$ be a finite abelian group and let $\widehat{G}$ be the set of irreducible characters $\chi: G \rightarrow \mathbb{C}$. $\widehat{G}$ is the dual group of $G$, where multiplication is defined as $(\chi_a \cdot \chi_b)(g) = \chi_a(g)\chi_b(g)$, and $\chi_a^{-1} = \overline{\chi_a}$. 

\begin{example}
	Trivial example: We have that for cyclic group $Z_n$, the set of characters from above is the dual group, and furthermore $\widehat{Z_n} \cong Z_n$. 
\end{example}

\begin{theorem}
	Let $G$ be a finite abelian group. Then $G \cong \widehat{G}$.
\end{theorem}

We can extend the definition of Fourier transforms on cyclic groups to any finite abelian group, so $\widehat{f}: \mathbb{Z}_n \rightarrow \mathbb{C}$ is defined as:
\begin{equation}
	\widehat{f}(\chi) = |G| \langle f, \chi \rangle = \sum_{g \in G} f(g) \overline{\chi(g)}
\end{equation}

Furthermore, we have that if $\chi, \psi \in \widehat{G}$, then $\widehat{\chi}(\psi) = \langle \chi, \psi \rangle = |G|\delta_{\chi, \psi}$. So $\widehat{\chi} = |G| \delta_{\chi}$, where $\delta_{\chi}(\psi) = 1$ if $\psi = \chi$ and 0 else. 

\begin{theorem}
	\label{thm:abelian dual commutation}
	Let $G$ be an abelian group, and $a, b \in \mathbb{C}G$. Then $ \widehat{a \ast b} = \widehat{a} \widehat{b}$. 
\end{theorem}

\begin{proof}
	We have that:
\begin{align*}
		\widehat{a \ast b}(\chi) &= n \langle a \ast b, \chi \rangle\\
		&= n 1/n \sum_{g \in G} (a \ast b) (g) \widehat{\chi(g)} \\
		&= \sum_{g \in G} \overline{\chi(g)} 
		\sum_{h \in G} a(g h^{-1}) b(h) \\
		&= \sum_{h \in G} b(h)  
		\sum_{g \in G} a(g h^{-1})\overline{\chi(g)}  
\end{align*} 
And setting $k = gh^{-1}$, so $g =kh$, we get that:
\begin{align*}
		&= \sum_{h \in G} b(h)  
		\sum_{k \in G} a(k)\overline{\chi(kh)} \\
		&= \sum_{h \in G} b(h)  
		\sum_{k \in G} a(k)\overline{\chi(k)} \overline{\chi(h)} \\
		&=  \sum_{h \in G} b(h)  \overline{\chi(h)} 
		\sum_{k \in G} a(k)\overline{\chi(k)}\\
		&= n \langle a, \chi \rangle n \langle b, \chi \rangle\\
		&= \widehat{a}(\chi) \widehat{b}(\chi)
\end{align*}
\end{proof}

\begin{theorem}
	\label{thm:Eigenvector operators}
	Let $G$ be a finite abelian group and $a \in \mathbb{C}(G)$. Define the operator $A: \mathbb{C}(G) \rightarrow \mathbb{C}(G)$ by $A(b) = a \ast b$. Then $A$ is linear and furthermore $\chi$ is an eigenvector with eigenvalue $\widehat{a}(\chi)$ for all $\chi \in \widehat{G}$. Therefore, $A$ is diagonalisable. 
\end{theorem}

\begin{proof}
	First, we have that $\widehat{a\ast \chi} = \widehat{a} \widehat{\chi} = \widehat{a} n \delta_\chi$, and $\widehat{a} n \delta_\chi(\psi) = \widehat{a}(\psi) n$ if $\psi = \chi$ and 0 else, so $\widehat{a} n \delta_\chi = \widehat{a}(\chi) n \delta_\chi$. When we apply the inverse of the Fourier transform, we have that $a \ast \chi = \widehat{a}(\chi) \chi$, so $A \chi = \widehat{a}(\chi) \chi$. Thus shown. 
\end{proof}

\section{Spectrums of random walks}
We define a convolution operator on a probability distribution $P$ as $M : L(G) \rightarrow L(G)$ where $M(a) = P \ast a$, where $(P \ast a) (g) =\sum_{h\in G} P(gh^{-1})a(h)$.
\begin{definition}
	The spectrum of a random walk driven by probability distribution $P$ is the set of eigenvalues of $M$. 
\end{definition} 
It is easy to show that $P \ast U = U$ and that $U$ is an eigenvector of $M$ with eigenvalue $1$. 


\begin{theorem}[Diaconis]
	\label{thm:Diaconis}
	Let $G$ be a finite abelian group and let $P$ be a probability on $G$. Then $spec(P) = \lbrace \widehat{P}(\chi) : \chi \in \widehat{G} \rbrace$ where the multiplicity of $\lambda \in spec(P)$ are the number of characters where $\widehat{P}(\chi) = \lambda$. The orthonormal basis for the eigenspace of $\lambda$ is the set of all characters $\chi$ where $\widehat{P}(\chi) = \lambda$. 
\end{theorem}
This is a special case of \cref{thm:Eigenvector operators}

\begin{lemma}
	\label{lem:Absolute Value Inequality}
	Define $\|f\| = \sqrt{\langle f, f \rangle}$, where $f \in \mathbb{C}(G)$. Then $\|f \|_1 \leq |G| \|f \|$.
\end{lemma}
\begin{proof}
	We have that $\|f \|_1 = |G| \langle f, \chi_1 \rangle \leq |G| \|f \| \|\chi_1 \| = |G| \|f \|$, where $\|\chi_1 \| = 1$ and we use the Cauchy-Schwartz inequality. 
\end{proof}

\begin{theorem}[Plancherel]
	\label{thm:Plancherel}
	Let $G$ be a finite abelian group and let $a, b \in \mathbb{C}(G)$. Then $\langle a, b \rangle = 1/|G| \langle \widehat{a}, \widehat{b} \rangle$. 
\end{theorem}
\begin{proof}
	We have that $a = 1/|G| \sum_{\chi \in \widehat{G}} \widehat{a}(\chi) \chi$, and $b = 1/|G| \sum_{\chi \in \widehat{G}} \widehat{b}(\chi) \chi$. Thus \begin{equation}
		\langle a, b \rangle = 1/|G|^2 \langle \widehat{a}(\chi) \widehat(b)(\chi) \delta_{\chi, \chi} = 1/|G| \langle \widehat{a}, \widehat{b} \rangle.
	\end{equation}

\end{proof}

\begin{lemma}[Diaconis–Shahshahan Upper bound lemma]
	\label{lem:Upper bound lemma}
	Let $G$ be a finite abelian group, and let $F$ be the set of non-trivial irreducible characters of $G$. Let $Q$ be a probability on $G$.  Then:
	\begin{equation}
		\|Q - U\|^2_{TV} \leq \frac{1}{4} \sum_{\chi \in F} |\widehat{Q}(\chi)|^2.
	\end{equation}
\end{lemma}
\begin{proof}[Proof]
	From \cref{prop:distribution difference equality}, we have that:
	\begin{equation}
		\|Q - U\|^2_{TV} = \frac{1}{4} \| Q - U\|_1^2
	\end{equation}
	and from \cref{lem:Absolute Value Inequality}, we have that:
	\begin{equation}
		\frac{1}{4} \| P - Q\|_1^2 \leq \frac{|G|^2}{4} \| Q - U \|^2
	\end{equation}
	Then by \cref{thm:Plancherel},
	\begin{equation}
		|G|^2 \|Q - U \|^2 = |G| \left[ \langle \widehat{Q}, \widehat{Q} \rangle - 2 \langle \widehat{Q}, \widehat{U} \rangle + \langle \widehat{U}, \widehat{U} \rangle \right]
	\end{equation}
	As $\widehat{U} = \delta_{\chi_1}$, then $\widehat{U}(\chi) = \langle \chi_1, \chi \rangle$. 
	Therefore, $\langle \chi_1, \chi_1 \rangle = 1/|G|$, and $\langle \widehat{Q}, \widehat{U} = \widehat{Q}(\chi_1) = 1/|G|$, and
	\begin{equation}
		\langle \widehat{Q}, \widehat{Q} \rangle = 1/|G| + 1/|G| \sum_{\chi \in F} \widehat{Q}(\chi) \overline{\widehat{Q}(\chi)}.
	\end{equation}
	Therefore, $|G|^2 \|Q - U \|^2 =  \sum_{\chi \in F}| \widehat{Q}(\chi)|^2$ and thus 
	\begin{equation}
		\|Q - U\|^2_{TV} \leq \frac{1}{4} \sum_{\chi \in F} |\widehat{Q}(\chi)|^2.
	\end{equation}
\end{proof}

\begin{corollary}
	\label{cor:Convolution Inequality}
	Let $G$ be a finite abelian group and let $P$ be a probability on $G$. Then 
	\begin{equation}
	\|P^{\ast k} - U\|^2_{TV} \leq \frac{1}{4} \sum_{\chi \in F} |\widehat{P}(\chi)|^{2k}.
\end{equation}
\end{corollary}

\begin{theorem}
	If $P$ is a probability on a finite abelian group $G$ and $P$ is ergodic, then $P^{\ast n}$ converges to $U$.
\end{theorem}
It suffices to show that $|\widehat{P}(\chi) < 1$ for all nontrivial characters $\chi$ of $G$. Assume that $P(g) > 0$ with ergodicity. Recall that $|\widehat{P}(\chi)|\leq \sum_{g \in G} P(g) |\overline{\chi(g)}| = 1$, from \cref{thm:Diaconis}. We have that if equality holds, then $P(g) \overline{\chi(g)}$ are non-negative multiples of a common complex number. But $P(1) \geq 0$ and $\overline{\chi(1)} = 1$. Therefore, $P(1) \overline{\chi(1)} = P(1)$, but we have that there is a $\overline{\chi(g)} \neq 1$ for some $g \in G$. Thus we have that equality does not hold, so $|\widehat{P}(\chi) < 1$. 

\end{document}
