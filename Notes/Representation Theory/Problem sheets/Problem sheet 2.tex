\documentclass[]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
%opening
\title{Representation Theory notes}
\author{Eric Luu}

\begin{document}

\maketitle

\section*{Chapter 1}
\subsection*{1.10}
We define $A_{B \rightarrow \tilde{B}}$ to be the matrix:
$
\begin{bmatrix}
	1 & 1 & 1\\
	1 & -1 & 0\\
	1 & 0 & -1
\end{bmatrix}
$.
The inverse of this matrix is:
$A_{\tilde{B} \rightarrow B} = (A_{B \rightarrow \tilde{B}})^{-1} = 
\begin{bmatrix}
	1/3 & 1/3 & 1/3\\
	1/3 & -2/3 & 1/3\\
	1/3 & 1/3 & -2/3
\end{bmatrix}
$
and $\tilde{\rho}_g = A_{B \rightarrow \tilde{B}} \rho_g A_{\tilde{B} \rightarrow B}$. 

We have that $\tilde{\rho}_{()}$ sends $(e_1 + e_2 + e_3)$ to $(e_1 + e_2 + e_3)$, $(e_1 - e_2)$ to $(e_1 - e_2)$, and $(e_1 - e_3)$ to $(e_1 - e_3)$. Therefore, $\tilde{\rho}_{()} = 
\begin{bmatrix}
	1 & 0 & 0\\
	0 & 1 & 0\\
	0 & 0 & 1
\end{bmatrix}
$.

We have that $\tilde{\rho}_{(12)}$ takes $e_1 + e_2 + e_3$ to $e_1 - e_2$, $e_1 - e_2$ to $e_1 + e_2 + e_3$ and $e_1 - e_3$ to $e_1 - e_3$. Therefore, 
$\tilde{\rho}_{(12)} = 
\begin{bmatrix}
	1 & 0 & 0\\
	0 & -1 & 0\\
	0 & -1 & 1
\end{bmatrix}
$.

We have that $\tilde{\rho}_{(23)}$ takes $e_1 + e_2 + e_3$ to $e_1 + e_2 + e_3$, $e_1 - e_2$ to $e_1 - e_3$, and $e_1 - e_3$ to $e_1 - e_2$. Therefore, 
$\tilde{\rho}_{(23)} = 
\begin{bmatrix}
	1 & 0 & 0\\
	0 & 0 & 1\\
	0 & 1 & 0
\end{bmatrix}
$.

$\tilde{\rho}_{(13)}$ takes $e_1 + e_2 + e_3$ to $e_1 - e_3$, takes $e_1 - e_2$ to $e_1 - e_2$, and $e_1 - e_3$ to $e_1 + e_2 + e_3$. Therefore, 
$\tilde{\rho}_{(13)} = 
\begin{bmatrix}
	1 & 0 & 0\\
	0 & 1 & -1\\
	0 & 0 & -1
\end{bmatrix}
$.
\\
$\tilde{\rho}_{(123)} = 
\begin{bmatrix}
	1 & 0 & 0\\
	0 & -1 & 1\\
	0 & -1 & 0
\end{bmatrix}
$.
\\
$\tilde{\rho}_{(132)} = 
\begin{bmatrix}
	1 & 0 & 0\\
	0 & 0 & -1\\
	0 & 1 & -1
\end{bmatrix}
$.
\\
We have that these matrices are decomposable into the spaces $\mathbb{R} \oplus \mathbb{R}^2$, thus $\rho$ is reducible by Matzche's theorem. We have that all of the nonzero entries are 1 or -1 and the determinant of the matrix equals the sign of the permutation.

\subsection*{1.11}
Recall that the left regular representation $\rho$ on a finite group $G$ is the representation on the vector space $V=$span$(\lbrace e_g : g \in G \rbrace)$ such that $\rho_g e_h = e_{gh}$ for all $g, h \in G$. However, consider the vector $v = \sum_{h \in G} e_h$. Then for all $g \in G$, $\rho_g v = \rho_g \sum_{h \in G} e_h = \sum_{h \in G} \rho_g e_h = \sum_{h \in G} e_{gh} = \sum_{x \in G} e_x = v$, thus the subspace span$(\lbrace v \rbrace)$ is an invariant subspace of $V$. However, if dim$(V) > 1$, then $\rho$ will have a nontrivial invariant subspace, thus $\rho$ is reducible. Therefore if $\rho$ is irreducible, then dim$(V) = 1$, therefore there is only a single element in $G$, which means that $G$ is the trivial group.
\subsection*{1.13}
We will first show that $\rho$ is reducible. We have that $\rho_a \begin{bmatrix}
	1\\
	0
\end{bmatrix}
= \begin{bmatrix}
	1\\
	0
\end{bmatrix}
$ for all $a$, therefore, the subspace span$\lbrace \begin{bmatrix}
	1\\
	0
\end{bmatrix} \rbrace$ is an invariant subspace in $(\mathbb{F}_p)^2$. 

To show $\rho$ is indecomposable, assume for contradiction that the span of $
\begin{bmatrix}
	1\\
	1\\
\end{bmatrix}$
invariant. Note that it is not in span$\lbrace \begin{bmatrix}
	1\\
	0
\end{bmatrix} \rbrace$.
However, we get that:
$
\begin{bmatrix}
	1 & 1\\
	0 & 1
\end{bmatrix}
\begin{bmatrix}
	1\\
	1\\
\end{bmatrix}
=
\begin{bmatrix}
	2\\
	1\\
\end{bmatrix}
$.
For all $p$, $1$ and $2$ are in different cosets for $\mathbb{Z}_p$. Thus, $\begin{bmatrix}
	2\\
	1\\
\end{bmatrix}$ is not in the span of $\lbrace \begin{bmatrix}
	1\\
	1
\end{bmatrix} \rbrace$. 
Therefore, $\rho$ is indecomposable by contradiction.

\section*{Chapter 2}
\subsection*{2.1}
\subsubsection*{a}
We shall show that $\lbrace v_1 \oplus 0, v_2 \oplus 0, ... v_m \oplus 0 \rbrace \cup \lbrace 0 \oplus w_1, 0 \oplus w_2, ... \rbrace$ is a basis for $V \bigoplus W$, by showing it spans $V \oplus W$ and is linearly independent. Let $v \oplus w \in V \bigoplus W$, where $v = \sum_i \alpha_i v_i$ and $w = \sum_j \beta_j w_j$, where $\alpha_i$ and $\beta_j$ are complex scalars. Then $\sum_i \alpha_i( v_i\oplus 0) + \sum_j \beta_j( 0 \oplus w_j) = (\sum_i \alpha_i v_i) \oplus 0 + 0 \oplus (\sum_j \beta_j w_j)$ by the direct sum scalar multiplication and addition relations. Finally, by the addition relation, we have that this is equal to $(\sum_i \alpha_i v_i) \oplus (\sum_j \beta_j w_j) = v \oplus w$, as required. Therefore, the set above spans $V \oplus W$. To show it is linearly independent, we have that if $\sum_i \alpha_i (v_i \oplus 0) + \sum_j \beta_j (0 \oplus w_j) = 0$, then $(\sum_i \alpha_i v_i) \oplus (\sum_j \beta_j w_j) = 0 \oplus 0$. However, as $v_i$ is linearly independent, and $w_j$ is linearly independent, it must hold that $\alpha_i = 0$ for all $i$, and $\beta_j = 0$ for all $j$. Thus the vectors are linearly independent, and the dimension of $V \oplus W$ is the size of the basis, which is dim $V + $ dim $W$.

\subsection*{b}
To show that $\rho \oplus \psi$ is a representation, we have that $(\rho \oplus \psi)_g (\rho \oplus \psi)_h (v \oplus w) = (\rho \oplus \psi)_g (\rho_h v \oplus \psi_h w)  = (\rho_g \rho_h v) \oplus (\psi_g \psi_h w) = (\rho_{gh} v) \oplus (\psi_{gh} w) = (\rho \oplus \psi)_{gh} v \oplus w$. Furthermore, we have that $(\rho \oplus \psi)_g (\alpha(v_1 \oplus w_1) + \beta(v_2 \oplus w_2)) = (\rho \oplus \psi)_g ((\alpha v_1 + \beta v_2) \oplus (\alpha w_1 + \beta w_2)) = (\rho_g (\alpha v_1 + \beta v_2) \oplus \psi_g (\alpha w_1 + \beta w_2)) = (\alpha \rho_g v_1 + \beta \rho_g v_2) \oplus (\alpha \psi_g w_1 + \beta \psi_g w_2)$, which is from $\rho$ and $\psi$ being representations. Finally, we have that $(\alpha \rho_g v_1 + \beta \rho_g v_2) \oplus (\alpha \psi_g w_1 + \beta \psi_g w_2) = \alpha (\rho_g v_1 \oplus \psi_g w_1) + \beta (\rho_g v_2 \oplus \psi_g w_2) = \alpha (\rho \oplus \psi)_g (v_1 \oplus w_1) + \beta (\rho \oplus \psi)_h (v_2 \oplus w_2)$. Thus $\rho \oplus \psi$ is a homomorphism and is linear, thus it is a representation. 

\subsection*{2.2}
\subsection*{a}
We will show the set spans $V \otimes W$ and is linearly independent.
Consider $v \otimes w \in V \otimes W$, and suppose $v = \sum_i \alpha_i v_i$ and $w = \sum_j \beta_j w_j$. Then $ (\sum_i \alpha_i v_i) \otimes (\sum_j \beta_j w_j) = \sum_i \alpha_i (v_i \otimes \sum_j \beta_j w_j)$ by the left linearity of the tensor product, and $\sum_i \alpha_i (v_i \otimes \sum_j \beta_j w_j) = \sum_i (\alpha_i \beta_j (v_i \otimes w_j))) = \sum_i \sum_j \alpha_i \beta_j (v_i \otimes w_j)$. Thus $(v_i \otimes w_j)$ spans $V \otimes W$. To show linear independence, consider the dual of $V \otimes W$. We have that the basis of the dual of $V$ is $\psi_i(v_j) = \delta_{i, j}$, and the basis of the dual of $W$ is $\rho_i(w_j) = \delta_{i,j}$. Then the map $\psi_{i,j}$ $V \times W \rightarrow \mathcal{C}$ by $v, w \rightarrow \psi_i(v) \rho_j(w)$ can be applied to the universal property to form a map $\psi_{i,j}(v \otimes w) = \psi_i(v) \rho_j(w)$. Thus the basis of the dual is $\psi_{i,j}$ and as this dual is the dual to the basis as stated, then this forms a map. 
\subsection*{b}
We have that $(\rho \otimes \psi)_g$ is linear by definition. Then  $(\rho \otimes \psi)_g (\rho \otimes \psi)_h (v \otimes w) = (\rho \otimes \psi)_g (\rho_h v \otimes \psi_h w) = (\rho_g \rho_h v \otimes \psi_g \psi_h w) = (\rho_{gh} v \otimes \psi_{gh} w) = (\rho \otimes \psi)_{gh} (v \otimes w)$, thus we have that this is a representation.

\subsection{2.3}
\subsubsection{Part a}
Let $\lbrace v_1, v_2, ..., v_m \rbrace$ be a basis for $V$. Then let $(w_1 \otimes w_2 \otimes ... \otimes w_k) \in \text{Sym}^k(V)$, and suppose $w_i = \sum_{j = 1}^n \alpha_{i,j} v_j$. Then we have that $w_1 \otimes w_2 \otimes ... \otimes w_k = \sum_{i} \alpha_{1, i} v_i \otimes w_2 \otimes ... \otimes w_k$. But we can repeat this multiple times to have that:
\begin{equation}
	w_1 \otimes w_2 \otimes ... \otimes w_k = \sum_{\sigma \in [n]^k} \prod_{i = 1}^k \alpha_{i,\sigma(i)} (v_{\sigma(1)} \otimes v_{\sigma(2)} \otimes ... \otimes v_{\sigma(k)})
\end{equation}
where $[n]^k$ is selecting $k$ items from $n$ with repetition in order. However, we have that $v_{\sigma(1)} \otimes v_{\sigma(2)}$ and since from the relation we can reorder every element, we can reorder $(v_{\sigma(1)} \otimes v_{\sigma(2)} \otimes ... \otimes v_{\sigma(k)})$ to be in nondecreasing order. As we can do this for all elements in the sum, then we can express $w_1 \otimes w_2 \otimes ... \otimes w_k$ as a sum of elements of the form. To show it is a basis, consider
\begin{equation}
	\sum_{\sigma \in [m]^k} A_{\sigma} (v_{\sigma(1)} \otimes v_{\sigma(2)} \otimes ... \otimes v_{\sigma(k)}) = 0
\end{equation}
If $k = 1$, then we have $\sum_{i = 1}^m A_{i} (v_{i}) = 0$, which implies $A_i = 0$ for all $i$ as $v_i$ is a basis of $V$. Suppose that for $k-1$, we have that
\begin{equation}
	\sum_{\sigma \in [m]^{k-1}} A_{\sigma} (v_{\sigma(1)} \otimes v_{\sigma(2)} \otimes ... \otimes v_{\sigma(k-1)}) = 0
\end{equation}
implies $A_{\sigma} = 0$ for all $\sigma$. Then we can express the $k$-th case as:
\begin{equation}
	 \sum_{k = 1}^m (\sum_{\sigma \in [v_{\sigma(k)}]^{k-1}} A_{\sigma}(v_{\sigma(1)} \otimes v_{\sigma(2)} \otimes ... v_{\sigma(k-1)}) \otimes v_{\sigma(k)}) = 0
\end{equation}
but that implies that $A_{\sigma} = 0$ for all $\sigma \in [v_{\sigma(k)}]^{k-1}$ from the inductive hypothesis, thus $A_{\sigma} = 0$ for all $\sigma \in [m]^k$.
\paragraph{Size of basis}
Therefore, the size of the bases is the number of these $v_{i_1} \oplus ... \oplus v_{i, k}$ where $1 \leq i_1 \leq ... \leq i_k \leq m$. However, there is a bijection between these bases and the number of ways to put $k$ unlabelled balls into $m$ bins, where the number of ways to arrange the balls and bins is $\binom{k + m - 1}{k}$. Therefore, $\text{dim Sym}^k (V) = \binom{\text{dim} V + k - 1}{k}$.
\subsubsection{Part b}
We have that $\text{Sym}^k(\rho)$ is linear by definition. What remains to show is that it is a homomorphism. Consider $\text{Sym}^k(\rho)_g \circ \text{Sym}^k(\rho)_h (v_1 \otimes v_2 \otimes ... \otimes v_k)$. Then we have this is equal to $\text{Sym}^k(\rho)_g (\rho_h v_1 \otimes \rho_h v_2 \otimes ... \otimes \rho_h v_k) = (\rho_g \rho_h v_1 \otimes \rho_g \rho_h v_2 \otimes ... \otimes \rho_g \rho_h v_k) = \text{Sym}^k(\rho)_{gh} (v_1 \otimes v_2 \otimes ... \otimes v_k)$. Thus $\text{Sym}^k(\rho)$ is a homomorphism. 
\end{document}
