\documentclass[]{article}
\usepackage[margin=1in]{geometry}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{url}

% Environments

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{conjecture}[theorem]{Conjecture}

\theoremstyle{definition}
\newtheorem{example}[theorem]{Example}

\numberwithin{theorem}{section}
\numberwithin{equation}{section}

\newcommand{\ops}{\overset{\text{ops}}{\leftrightarrow}}

%opening
\title{Combinatorics Notes}
\author{Eric Luu}

\begin{document}

\maketitle
\section{Asymptotics}

We like counting things! But what about big things? How can we estimate what happens when numbers go big?

We say $a_n \sim b_n (n \rightarrow \infty)$ if $\lim_{n \rightarrow \infty} \frac{a_n}{b_n} = 1$. 
Example: $n! \sim \left(\frac{n}{e}\right)^n \sqrt{2 \pi n}$. 

We say $a_n = O(b_n)$ if there is $c$ such that $|a_n| \leq c |b_n|$ when $n$ is sufficiently large. 

We say $a_n = o(b_n)$ if $\frac{a_n}{b_n} \rightarrow 0$ as $n \rightarrow \infty$. 

Note that $=$ is not the same ``equals'' that we're used to.

Simple case:
Let $c_n = n - o(n)$, which says ``there exists a function $g(n)$ where $g(n) = o(n)$ such that $c_n = n - g(n)$''.

\begin{lemma}[Reciprocals lemma]
	If $f(n) \rightarrow 0$ then $\frac{1}{1 + f(n)} = 1 + O(f(n))$. 
\end{lemma}

This brings us to the most important rule:
\begin{theorem}[Asymptotic equivalence]
	$f(n) \sim g(n)$ iff $f(n) = g(n)(1 + o(1))$.
\end{theorem}

\subsection{Rules for $O$}
\begin{enumerate}
	\item $O(f(n)) + O(f(n)) = O(f(n))$ and $o(f(n)) + o(f(n)) = o(f(n))$
	\item $a O(f(n)) = O(f(n))$ and $a o(f(n)) = o(f(n))$ when $a$ is constant. 
	\item Functions distribute, so $g(n) O(f(n)) = O(g(n) f(n))$ and $g(n) o(f(n)) = o(g(n) f(n))$. Furthermore, $O(g(n) f(n)) = g(n) O(f(n))$ and  $o(g(n) f(n)) = g(n) o(f(n))$. 
	\item $o(f(n)) = O(f(n))$, $o(O(f(n))) = o(f(n))$, $O(o(f(n))) = o(f(n))$. 
	\item If $f(n) \sim g(n)$ and $g(n) \sim h(n)$ then $f(n) \sim h(n)$. 
	\item If $f= O(g)$ then for all functions $h$, $h(f) = h(O(g))$. 
	\item If $f= o(g)$ then for all functions $h$, $h(f) = h(o(g))$. 
\end{enumerate}

Very important! $f_1 \sim g_1$ and $f_2 \sim g_2$ does not imply $f_1 + f_2 \sim g_1 + g_2$. 
\subsection{Taylor's theorem}
Suppose $f^{(k)}$ exists on $[0, x]$ where $x > 0$. Then:
\begin{equation}
	f(x) = \sum_{i = 0}^{k - 1} \frac{1}{i!}f^{(i)}(0) x^i + \frac{1}{k!} f^{(k)}(\zeta) x^k
\end{equation}

where $0 \leq \zeta \leq x$. Then if $f^(k)$ is bounded around $[0, x_1]$ for some $x_1 > 0$ then:
\begin{equation}
	f(x) = \sum_{i = 0}^{k - 1} \frac{1}{i!}f^{(i)}(0) x^i + O(x^k) \quad \text{for} \, x \in [0, x_1]
\end{equation}

\subsubsection{Special cases:}

\begin{equation}
	\frac{1}{1-x} = 1 + x + x^2 + O(x^3) \quad \text{on } [-1/2, 1/2]
\end{equation}
For $f(n) = o(1)$, $f(n) \in [-1/2, 1/2]$ for large $n$, we have that:
\begin{equation}
	\frac{1}{1-f(n)} = 1 + f(n) + f(n)^2 + O(f(n)^3) \quad \text{on } [-1/2, 1/2]
\end{equation}
or we can go to higher precision. 

Similarly:
\begin{equation}
	\log( 1 + f(n)) = f(n) - \frac{1}{2} f(n)^2 + O(f(n)^3)
\end{equation}
So $\log(1 + o(1)) = o(1)$

We also have that $e^{o(1)} = 1 + o(1)$. 

Therefore, $1 + o(1) = e^{o(1)} = 1 + o(1)$. 

Take care with unbounded sums! we want to have that the summations aer bounded above to go to 1. 

\subsection{Uniformity}
We say $f_i(n) = O(g(n))$ uniformly for $i \in S_n$ if there is a $c$ and $n_0$ such that $|f_i(n)| \leq c |g(n)|$ for all $i \in S_n$ when $n \geq n_0$. 
Alternatively, we could replace $c$ with a function $h(n)$ where $h(n) \rightarrow 0$. 

\section{Example: Extracting binomial coefficients}
Let us estimate $\binom{n}{\lfloor \frac{n}{2} \rfloor}$. Suppose $a = \lfloor \frac{n}{2} \rfloor = \frac{a}{2} + O(1)$. 
We can compare $\binom{n}{a}$ with $\binom{n}{a + b}$. We have that:

\begin{equation}
	\binom{n}{a + i} = \binom{n}{a + i - 1} \frac{n - (a + i - 1)}{a + i}
\end{equation}
Let $R_i = \binom{n}{a + i}/ \binom{n}{a + i - 1} = \frac{n - (a + i - 1)}{a + i}$.

Then for $b > 0$, we have that:
\begin{equation}
	\binom{n}{a + b}/ \binom{n}{a } = \prod_{i = 1}^b R_i
\end{equation}
From above, we have that:
\begin{align*}
	R_i = \frac{n - a - i + 1}{a + i}\\
	&= \frac{n - n/2 + O(1) - i}{n/2 + O(1) + i}\\
	&= \frac{n/2 + O(1) - i}{n/2 + O(1) + i}\\
	&= \frac{1 + O(1/n) - 2i/n}{1 + O(1/n) + 2i/n}
\end{align*}
Now let $b = o(n^{2/3})$, and we have that $1 \leq 1 \leq b$. Then $2i/n + O(1/n) \rightarrow 0$ as $n \rightarrow \infty$. Using Taylor, we have that $\log(1 + x) = x + O(x^2)$, so taking the log of $R_i$ we get:
\begin{align*}
	\log R_i &= \log(1 - 2i/n + O(1/n))- \log(1 + O(1/n) + 2i/n)\\
	&= -2i/n + O(1/n) + O(i^2/n^2) - 2i/n + O(1/n) + O(i^2/n^2)\\
	&= -\frac{4i}{n} + O(1/n + i^2/n^2)\\
	&= -\frac{4i}{n} + O(1/n + b^2/n^2)\\
\end{align*}

Therefore, taking the sum of logs yields:
\begin{align*}
\sum_{i = 1}^b \log R_i &= 	\sum_{i = 1}^{b} - 4i/n + O(1/n + b^2/n^2)\\
&= \left(..\sum_{i = 1}^{b} - 4i/n\right) + O(b/n + b^3/n^2)\\
&= \frac{-2b^2 + O(b)}{n} + O(b/n + b^3/n^2)\\
&= \frac{-2b^2 + O(b)}{n} + o(1)
\end{align*}
Then $\binom{n}{a + b}/ \binom{n}{a} = \exp(-2b^2/n + o(1)) \sim e^{-2b^2/n}$. 
This gives us a uniform bound over $b$. 

\section{Asymptotics of summations}

Suppose we select a subset $T$ of $[n]$ with uniform probability. How many elements will we get?

$\mathbb{P}(|T| = k) = \frac{1}{2^n} \binom{n}{k}$ So expected number is $\mathbb{E}(|T|) = \frac{1}{2^n} \sum_{k = 0}^{n} k \binom{n}{k}$.

Let $S_n = \sum_{k = 0}^{n} k \binom{n}{k}$. Then $\sum_{k = 0} \binom{n}{k} x^k = (1 + x)^n$. Differentiating wrt with $x$, we get $\sum_{k = 1}^{n} \binom{n}{k} k x^{k - 1} = n (1 + x)^{n - 1}$. Set $x = 1$, we get $S_n = n 2^{n - 1}$, so the expected size of $|T| = n/2$. 

\subsection{Exercise}
Find
$\sum_{k = 0}^{\infty} \frac{k^2}{2^k}$.
\subsection{Number of derangements}
We have that $D_n = n! \sum_{i = 0}^{n} (-1)^i/i!$. We know $\sum_{i = 0}^{n} (-1)^i/i! = e^{-1}$ as $n \rightarrow \infty$, so $D_n = n!/e(1 + o(1))$. so $D_n \sim n!/e$. 


\section{Bonferroni inequalities}
Recall $N_=(T)$ is the number of elements with exactly $T$ properties. Then we have that for any integer $m \geq 0$:
\begin{equation}
	N_=(T) \leq \sum_{\substack{Y \supseteq T \\ |Y| \leq |T| + 2m}} (-1)^{|Y - T|}N_{\geq }(Y)
\end{equation}
and 
\begin{equation}
	N_=(T) \geq \sum_{\substack{Y \supseteq T \\ |Y| \leq |T| + 2m + 1}} (-1)^{|Y - T|}N_{\geq }(Y)
\end{equation}
Essentialy, the Main Bonneferoni Implication is:
\begin{equation}
	\left| N_=(T) - \sum_{\substack{Y \supseteq T \\ |Y| \leq |T| + k}} (-1)^{|Y - T|}N_{\geq }(Y) \right|  \leq  \sum_{\substack{Y \supseteq T \\ |Y| \leq |T| + k + 1}}N_{\geq }(Y)
\end{equation}

\subsection{Proof}
We look at the contribution that each individual element has on the whole. 
The proof relies on the fact that:
$R_a = \sum_{k = 0}^{2m} (-1)^k \binom{q}{k} \geq L_a$. 

We have that when $L_a = 1$, then $R_a = 1$. When $L_a = 0$, we want $R_a \geq 0$. 

What happens if $2m \leq q/2$? Since it is increasing then this is $\geq 1$.

If $q/2 \geq 2m \geq q$, then we pair up the $2m + k$ terms with a $k$ term to get something that is bigger than 0.

Thus the proof falls into place, and a similar proof can be used for $2m + 1$. 

\subsection{Permutations with no 3-cycles}
Let $P_{(i,j,k)}$ be the property that a permutation $\sigma$ contains $C_i$. Want no 3-cycles. set $S$ be set of all properties, $T = \emptyset$. By MBI, the estimate is:
\begin{equation}
	\sum_{\substack{Y \subseteq S}}^{Y \leq k} (-1)^{|Y|} N_\geq(Y)
\end{equation}

If we have $j$ 3-cycles which are all disjoint, then $N_\geq(Y) = (n - 3j)!$ and the number of 3-cycles with $F(Y)$ disjoint is $(n - o(n))^3/3 \sim n^3/ 3$. Therefore, step $1$ has $(n - o(n))^3/3^j \sim n^{3j}/3^j$. Thus, 

$\sum_{Y, |Y| = j} N_\geq (Y) \sim n^{3j}/3^j (n - 3j)!/j! \sim n!/3^j j!$

Then by Poisson principle, we have that $N_=(\emptyset)/n! \sim e^{- 1/3}$. 

\section{Summations}
\subsection{Asymptotics of regular summations}

\begin{theorem}[Uniform Summation]
	Suppose $s_{i, n} \sim t_{i, n}$ for $ 1\geq i \geq I(n)$ where the asymptotics are uniform. So for $1 \leq i \leq I$ we have that:
\begin{equation}
	\frac{s_{i, n}}{t_{i, n}} = 1 + o(1)
\end{equation}

Then:
\begin{equation}
	\sum_{i = 1}^{I(n)} s_{i, n} \sim \sum_{i = 1}^{I(n)} t_{i, n}
\end{equation}
\end{theorem}

\begin{theorem}[Non-uniform summations]
	Suppose $s_{i, n} \sim t_{i, n}$ for $ 1\geq i \geq I(n)$ where the asymptotics are not uniform. So for $1 \leq i \leq I$ we have that:
	\begin{equation}
		\frac{s_{i, n}}{t_{i, n}} = 1 + O(g(n))
	\end{equation}
	
	Then:
	\begin{equation}
		\sum_{i = 1}^{I(n)} s_{i, n} \sim 1 + O(g(n))\sum_{i = 1}^{I(n)} t_{i, n}
	\end{equation}
\end{theorem}

\subsection{Union bound}
Given sets $S_1 ..., S_k$ we have that:
\begin{equation}
	|S_1 \cup ... \cup S_k | \leq \sum_{i = 1}^k |S_i
\end{equation}
Good bound
\subsection{Poisson Paradigm}

Recall that $[x]_n = x( x - 1) (x - 2) ... (x - n + 1)$. 
\begin{theorem}[Poisson Paradigm]
	In the setting of PIE, suppose $N = N_\geq(\emptyset) = |A|$ and $M = |S|$. Then suppose for some $\lambda = \lambda(n)$, where $\lambda = O(1)$, and we have for each fixed $k \geq 0$ either:
	\begin{equation}
		\sum_{\substack{Y \supseteq T\\ |Y| = |T| + k}} N_{\geq}(Y) = N \frac{\lambda^r}{[M]_r} \left(\frac{\lambda^k}{k!} + o(1)\right)
	\end{equation}
	uniformly for all $T \subset S$ with $|T| = r$ or:
	\begin{equation}
		\sum_{T = r} \sum_{\substack{Y \supseteq T\\ |Y| = |T| + k}} N_{\geq}(Y) = N \frac{\lambda^r}{r!} \left(\frac{\lambda^k}{k!} + o(1)\right)
	\end{equation}
	
	Then:
	\begin{equation}
		\frac{1}{N}\sum_{|T| = r} N_{\geq}(Y) = \frac{e^{-\lambda}\lambda^r}{r!} + o(1)
	\end{equation}
	
\end{theorem}

\subsection{Approximate Counting Lemma}
\begin{theorem}
	Let $N_1, ..., N_k$ be functions of $n$ where $k = k(n)$. Suppose the number of ways of performing ``action'' i is $N_i( 1 + O(\zeta_i))$ where $O()$ is uniform. Also assume $N_i(1 + O(\zeta))$ are strictly positive. Then the number of ways I can perform all actions is:
	\begin{equation}
		\exp\left(O\left(\sum_{i = 1}^{k}\zeta_i\right)\right) \prod_{i = 1}^k N_i
	\end{equation}
	if $\zeta_i = o(1)$ uniformly for all $i$, and at most:
		\begin{equation}
		\exp\left(O\left(\sum_{i = 1}^{k}\zeta_i\right)\right) \prod_{i = 1}^k N_i
	\end{equation}
	in all cases. 
\end{theorem}

\subsection{Estimation using integrals}
Suppose the asymptotics look something like this
\begin{equation}
	f(n) \sim c\sum_{k = -\infty}^{\infty} e^{-g(n) k^2}
\end{equation}
where $c$ is constant and $g(n) = o(1)$, say $g = \frac{1}{\log^2 n}$.

When can we replace the sum by an integral?

\subsection{Integral-sum lemma}
\begin{lemma}
		Suppose $g'(x)$ is continuous on $[a, b + 1]$. Then
	\begin{equation}
		\left| \sum_{n = a}^{b} g(n) - \int_{a}^{b + 1}g(x)\, dx\right| \leq \sum_{k = a}^{b} \max_{k \leq x \leq k + 1} |g'(x)|
	\end{equation}
	From triangle estimation of the Riemannian integral. 
\end{lemma}

\begin{corollary}
	If $g'(x)$ is continuous on $[a, b + 1]$ then
	\begin{equation}
		\sum_{n = a}^{b} g(n)  = \int_{a}^{b + 1}g(x) \, dx+ O\left( \sum_{k = a}^{b} \max_{k \leq x \leq k + 1} |g'(x)| \right)
	\end{equation}
\end{corollary}

An important integral for estimating these sums is the Gaussian integral.

\begin{equation}
	\int_{-\infty}^\infty e^{- \alpha x^2} \, dx = \sqrt{\frac{\pi}{\alpha}}
\end{equation}

An important lemma with the Gaussian is the exp-sum lemma.
\begin{lemma}
	If $\alpha = \alpha(n) = o(1)$, then
\begin{equation}
	\sum_{k = -\infty}^\infty e^{- \alpha k^2} = \sqrt{\frac{\pi}{\alpha}}
\end{equation}
\end{lemma}

In fact, we can go one step further, and we can figure out when the tails do not matter.

\begin{theorem}[Exponential tails lemma]
	If $\alpha \rightarrow 0$ and $M \sqrt{\alpha} \rightarrow \infty$ as $n \rightarrow \infty$ then:
	\begin{equation}
		\sum_{k = - M}^{M} e^{- \alpha k^2} \sim \sum_{k = - \infty}^{\infty} e^{- \alpha k^2}
	\end{equation}
\end{theorem}
We wish to show that:
\begin{equation}
	\sum_{k = M + 1}^{\infty} e^{- \alpha k^2} = o(\sqrt{\frac{\pi}{\alpha}})
\end{equation}
By comparing the integrals and summation, we have that:
\begin{equation}
	\sum_{k = M + 1}^{\infty} e^{- \alpha k^2} < \int_{ M }^{\infty} e^{- \alpha k^2} \, dx
\end{equation}
and by doing a change of variables $u = x \sqrt{\alpha}$ for fixed $\alpha$, we have that:
\begin{equation}
	\sum_{k = M + 1}^{\infty} e^{- \alpha k^2} < \int_{ M \sqrt{\alpha}}^{\infty} e^{- u^2} \, du/\sqrt{\alpha}
\end{equation}
And since $M \sqrt{\alpha} \rightarrow \infty$ and $\int_{ -\infty}^{\infty} e^{- u^2} \, du/ = \sqrt{\pi}$ is bounded, then

\begin{equation}
	 \int_{ M \sqrt{\alpha}}^{\infty} e^{- u^2} \, du/\sqrt{\alpha} = o(1)
\end{equation}
Thus the tails is small, therefore convergence. 

\subsection{Stirling's formula}
Consider $n!$. Taking the logs, we get that 
\begin{equation}
	\log(n !) = \sum_{k = 1}^{n} \log k
\end{equation}
Now we have from the naive integral-sum lemma above that:
\begin{align*}
	\left|\sum_{k = 1}^{n} \log k - \int_1^{n + 1} \log x \, dx\right| &\leq \int \sum_{k = 1}^n \max_{k \leq x \leq k + 1} \left(\frac{1}{x}\right)\\
	&\leq \sum_{k = 1}^n \frac{1}{k} = O(\log n)
\end{align*}

So we have that $n! = \exp\left( \left(\int_1^{n + 1} \log x \, dx\right) + O(\log n)\right) = O(n)(n \exp n - \exp n + 1)$. But this bound is not tight asymptotically! We want a tighter bound. 

Let $\delta_k$ be the error in the trapezoidal rule,

\begin{equation}
	\int_k^{k + 1}\log x \, dx = \frac{1}{2}(\log k + \log(k + 1)) + \delta_k
\end{equation}

It is known by Taylor that if $f$ is twice differentiable, the error $\delta_k$ is at most
\begin{equation}
	\frac{1}{12} f''(\zeta) \, \text{ for some } \zeta \in (k, k + 1)
\end{equation}

which is less than $\frac{1}{k^2}$ for all $k$ Then we have that:

\begin{equation}
	\int_1^n \log x \, dx = \sum_{k = 2}^{n - 1} \log k + \frac{1}{2}(\log 1 + \log n) + \sum_{k = 1}^{n - 1} \delta_k
\end{equation}
As $\sum_{k = 1}^{n - 1} \delta_k$ and bounded above by $\sum_{k = 1}^{\infty} \frac{1}{k^2} = \frac{\pi}{6}$, then $\sum_{k = 1}^{n - 1} \delta_k \rightarrow c_0$ as $n \rightarrow \infty$ for some constant $c_0$. 

Therefore, we have that:
\begin{align*}
	\int_1^n \log x \, dx &= \log (n - 1)! + \frac{1}{2}\log n + c_0 + o(1)\\
	\log (n - 1)! &= n \log n - n + 1 - \frac{1}{2}\log n - c_0 + o(1)\\
\end{align*}
Taking the exponential, we get:

\begin{equation}
	n! = \left(\frac{n}{e}\right)^n \sqrt{n} e^{n - c_0 + o(1)}
	\sim \left(\frac{n}{e}\right)^n \sqrt{n} c_1
\end{equation}
where $c_1 = e^{1 - c_0}$ is a constant.

How to compute $c_1$? We can do it by
\subsubsection{Summing binomial constants}
We let $a = \left\lfloor \frac{n}{2} \right\rfloor$.

Now we use the above formula to compute:
\begin{align*}
	\binom{n}{a} = \frac{n!}{a! (n - a)!} &\sim \frac{\left(\frac{n}{e}\right)^n c_1 \sqrt{n}}{\left(\frac{a}{e}\right)^a c_1 \sqrt{a} \left(\frac{n - a}{e}\right)^{n - a} c_1 \sqrt{n - a}}\\
	&\sim \frac{n^n}{a^a (n - a)^{n - a}}\frac{2}{c_1 \sqrt{n}}
\end{align*}

Now either $a = \frac{n}{2}$ or $a = \frac{n}{2} - \frac{1}{2} = \frac{n}{2} + \delta$ where $\delta = 0, -\frac{1}{2}$. 
Similarly, $n - a = \frac{n}{2} + \delta$ where $\delta = 0, \frac{1}{2}$.

To analyse:
\begin{equation}
	\left(\frac{n}{2} + \delta\right)^{\frac{n}{2} + \delta}
\end{equation}
we use the logarithmic trick:

\begin{align*}
	\left(\frac{n}{2} + \delta\right)^{\frac{n}{2} + \delta} &= \left(\frac{n}{2} \right)^{\frac{n}{2} + \delta}\left(1 + \frac{2 \delta }{n}\right)^{\frac{n}{2} + \delta}\\
	&= \left(\frac{n}{2} \right)^{\frac{n}{2} + \delta} e^{\left(\frac{n}{2} + \delta\right) \log( 1 + \frac{2 \delta_k}{n})}\\
	&= \left(\frac{n}{2} \right)^{\frac{n}{2} + \delta}e^{\left(\frac{n}{2} + \delta\right)\left(\frac{2\delta}{n} + O\left(\frac{1}{n^2}\right)\right)}\\
	&= 	\left(\frac{n}{2} \right)^{\frac{n}{2} + \delta}e^{\delta + o(1)}
\end{align*}
	Therefore, $a^a \sim  \left(\frac{n}{2} \right)^{\frac{n}{2} - \delta}e^{-\delta + o(1)}$ where $\delta = 0 , \frac{1}{2}$ and 
	$(n - a)^{n - a} \sim \left(\frac{n}{2} \right)^{\frac{n}{2} + \delta}e^{\delta + o(1)}$ with the same $\delta$.
	Therefore:
	\begin{equation}
		\binom{n}{a} = \frac{n^n}{\left(\frac{n}{2} \right)^{\frac{n}{2} - \delta}e^{-\delta} \left(\frac{n}{2} \right)^{\frac{n}{2} + \delta}e^{\delta}}\frac{2}{c_1 \sqrt{n}} = 2^n \frac{2}{c_1 \sqrt{n}}
	\end{equation}
	
	Now recall that for $b = o(n^{2/3})$ that
	\begin{equation}
		\frac{\binom{n}{a + b}}{\binom{n}{a}} = e^{-2 b^2/n}(1 + O(\frac{b}{n} + \frac{b^3}{n^2}))
	\end{equation}
	uniformly. So putting $M = \left\lfloor n^{3/5}\right\rfloor$ we have that:
	\begin{align*}
		\sum_{b = - M}^M \binom{n}{a + b} &= \binom{n}{a} \sum_{b = - M}^{M}e^{-2b^2/n}( 1 + O(n^{-1/5}))\\
		&\sim 2^n \frac{2}{c_1 \sqrt{n}} \sum_{b = - M}^{M}e^{-2b^2/n}
	\end{align*}
	We wish to show that 
	\begin{equation}
		\sum_{b = - M}^M \binom{n}{a + b} \sim 2^n
	\end{equation}
	so it suffices to show
	\begin{equation}
		\sum_{k < a- M} \binom{n}{k} = o(2^n)
	\end{equation}
	Recall $M = \left\lfloor n^{3/5}\right\rfloor$. Then taking ratios, we have that
	\begin{equation}
		\frac{\binom{n}{k - 1}}{\binom{n}{k}} = \frac{k}{n - k + 1}< 1
	\end{equation}
	as $k < \frac{n}{2}$. So the max term is at most $k =  a - M$.
	So:
	\begin{align*}
	\sum_{k < a- M} \binom{n}{k} = o(2^n) &< n \binom{n}{a - M}\\
	&\sim  n e^{2M^2/n}\binom{n}{a}
\end{align*}
As $M^2 \sim n^{6/5}$, $\log(n e^{2M^2/n}) = \log(n) - 2 n^{1/5}(1 + o(1))$ which goes to $- \infty$, so 
	\begin{equation}
	\sum_{k < a- M} \binom{n}{k} = o\left(\binom{n}{a}\right) = o(2^n)
\end{equation}
Thus the claim is proven.

Therefore, we have that 
\begin{align*}
	2^n \sim 2^n \frac{2}{c_1 \sqrt{n}} \sum_{b = - M}^{M}e^{-2b^2/n}\\
	c_1 \sim \frac{2}{\sqrt{n}} \sum_{b = - \infty}^{\infty}e^{-2b^2/n}\\
	\sim \frac{2}{\sqrt{n}} \sqrt{\frac{n\pi}{2}}\\
	\sim \sqrt{2 \pi}
\end{align*}
Where the above comes from the fact that $M \sqrt{\frac{2}{n}} \rightarrow \infty$ and the exponential tails lemma.

\subsection{Bootstrapping and asymptotic analysis}
Consider the equation $w = w(n)$ and $w \log(w + 1) = n$. Suppose further that $w \in [1, n]$. What is the asymptotic growth of $w$?

Is this solution unique? Yes, $\log(w + 1)$ is growing. 

The trick: We rearrange to be:

\begin{equation}
	w + 1 = \frac{n}{\log( w + 1 )} + 1
\end{equation}

We first plug in the bounds $\frac{n}{\log 2} \geq \frac{n}{\log(w + 1)} = w \geq \frac{n}{\log(n + 1)}$. 

Then we add 1 and take logs.
\begin{equation}
	\log(w + 1) \leq \log(\frac{n}{\log 2} + 1) < \log n + 1
\end{equation}
and
\begin{equation}
	\log(w + 1) > \log n - \log \log (n + 1)
\end{equation}
Therefore, 

$\log(w + 1) = \log n (1 + O(\frac{\log \log n}{\log n}))$
Plugging into the RHS, we have
\begin{align*}
	w + 1 &= \frac{n}{\log n(1 + O(\frac{\log \log n}{\log n}))} + 1\\
	&= \frac{n}{\log n}\left(1 + O(\frac{\log \log n}{\log n})\right) + 1\\
\end{align*}

Then taking the logs of both sides, we get more logs! but we get closer asymptotically.

\section{Analytic combinatorics}

Suppose we have an ordinary generating function $a_n$ with coefficients $\left\{a_n\right\}_{n \geq 0}$. 

Suppose $\sum_n a_n x^n$ converges for all $|x| \leq 1/2$. What does this tell us about $a_n$?
\begin{lemma}
	Suppose $f_(x_0)$ converges in the interval $[0, x_0]$ and $[x^n] f(x) \geq 0$ for all $n \geq 0$. Then 
	\begin{equation}
		[x^n] f(x) \leq x_0^{-n} f(x_0).
	\end{equation}
\end{lemma}
As an example, consider $a(x) = e^x$. Using the lemma, we have that 
\begin{equation}
	\frac{1}{n!} \leq x^{-n} e^x
\end{equation}
But what is the best bound for $1/n!$? Taking $D \log$, we have that $(- n \log x + x)' = - n/x + 1 = 0$. So $x = n$ is the best solution, yielding
\begin{equation}
	n! \geq \left(n/e\right)^n
\end{equation}
as an exact bound. This is very close to $\left(n/e\right)^n \sqrt{2 \pi n}$, which is Stirling's approximation. 

This is known as the saddlepoint bound. If we treat this as living on the complex plane, this number is the smallest real number to fulfil the bound, but moving slightly up/down the imaginary plane yields smaller values. 

We now move onto complex analysis.

\subsection{Complex analysis}
Let $f(x) = \sum a_n x^n$ be a formal power series. Now suppose $S$ is the $\limsup |a_n|^{1/n}$ or the supremum of the set of $w$ such that $|a_n|^{1/n} > w i.o$. 
of course $S = \infty$ if $|a_n|^{1/n}$ is unbounded.

\begin{theorem}
	Put $R = 1/S$. Then $f(z)$ converges for all $z$ with $|z| < R$. $f(z)$ diverges for all $z$ with $|z| > R$. On $R$ it may converge or diverge, but typically there is only a countable number of divergent points. 
\end{theorem}
In fact we have that for all $\varepsilon > 0$:
\begin{equation}
	|a_n | < (\frac{1}{R} + \varepsilon)^n 
\end{equation}
eventually, and 
\begin{equation}
	|a_n | > (\frac{1}{R} - \varepsilon)^n 
\end{equation}
i.o.
\end{document}
